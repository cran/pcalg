%\documentclass[article]{jss}
\documentclass[nojss,shortnames,article]{jss}
%              ----- for the package-vignette, don't use JSS logo, etc
%  shortnames: just because we don't want to see 22 author names for {HughesEtAl00}
% but when submitting, do

\usepackage[utf8]{inputenc}

% get rid of too much vertical space between R input & output:
\fvset{listparameters={\setlength{\topsep}{0pt}}}
%\renewenvironment{Schunk}{\vspace{\topsep}}{\vspace{\topsep}}
%
\usepackage{algorithmic}
\usepackage{algorithm}

%% Mathematics
\usepackage{amsmath}

%% Graphics
\usepackage{tikz}
\usetikzlibrary{snakes,arrows,shapes,decorations.markings}
\usepackage{xstring}

%\VignetteIndexEntry{Causal Inference: The R package pcalg}
%%\VignetteDepends{pcalg, sfsmisc, Rgraphviz}
\SweaveOpts{engine=R,eps=FALSE,pdf=TRUE,width=7,height=4}
\SweaveOpts{keep.source=TRUE,strip.white=true, figs.only=TRUE}
%           ^^^^^^^^^^^^^^^^ preserve comments (and all) in R chunks

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% declarations for jss.cls %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% almost as usual
\newcommand*{\AUT}[1]{{\normalsize #1} \\ {\small ETH Zurich}}
\author{
  \AUT{Markus Kalisch} \And
  \AUT{Martin M채chler} \And
  \AUT{Diego Colombo} \AND
  {\normalsize Alain Hauser} \\ {\small University of Bern} \And
  \AUT{Marloes H. Maathuis} \And
  \AUT{Peter B체hlmann}}
%JSS:\title{Causal Inference using Graphical Models with the \proglang{R} Package \pkg{pcalg}}
%% Slightly modified title, to better distinguish from JSS paper:
\title{More Causal Inference with Graphical Models in \proglang{R} Package \pkg{pcalg}}
%% for pretty printing and a nice hypersummary also set:
\Plainauthor{Kalisch, M채chler, Colombo, Hauser, Maathuis, B체hlmann} %% comma-separated
%JSS:\Plaintitle{Causal Inference using Graphical Models: The R package pcalg} %% without formatting
\Plaintitle{More Causal Inference with Graphical Models in R Package pcalg}
\Shorttitle{More Causal Graphical Models: Package pcalg} %% a short title (if necessary)

%% an abstract and keywords
\Abstract{
  The \pkg{pcalg} package for \proglang{R} \citep{citeR} can be
  used for the following two purposes: Causal structure learning and
  estimation of causal effects from observational and/or interventional
  data. In this document, we give a brief overview of the methodology, and
  demonstrate the package's functionality in both toy examples and
  applications.

  This vignette is an updated and extended (FCI, RFCI, etc) version of
  \cite{KalMMCMB:2012} which was for \pkg{pcalg} 1.1-4.
}

\Keywords{IDA, PC, RFCI, FCI, GES, GIES, do-calculus, causality, graphical model, \proglang{R}}
\Plainkeywords{IDA, PC, RFCI, FCI, GES, GIES, do-calculus, causality, graphical models, R} %% without formatting
%% at least one keyword must be supplied

%% publication information
%% NOTE: Typically, this can be left commented and will be filled out by the technical editor
%% \Volume{13}
%% \Issue{9}
%% \Month{September}
%% \Year{2004}
%% \Submitdate{2004-09-29}
%% \Acceptdate{2004-09-29}

%% The address of (at least) one author should be given
%% in the following format:
\Address{
  Markus Kalisch\\
  Seminar f\"ur Statistik\\
  ETH Z\"urich\\
  8092 Z\"urich, Switzerland\\
  E-mail: \email{kalisch@stat.math.ethz.ch}\\
  %% URL: \url{http://stat.ethz.ch/people/kalisch}
}
%% It is also possible to add a telephone and fax number
%% before the e-mail in the following format:
%% Telephone: +43/1/31336-5053
%% Fax: +43/1/31336-734

%% for those who use Sweave please include the following line (with % symbols):
%% need no \usepackage{Sweave.sty}

%% end of declarations %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Abbreviations, definitions, etc.
\DeclareMathOperator{\doop}{do}

%% Different edge types in graphs
\newlength{\edgelength}
\setlength{\edgelength}{3.5ex}
%\DeclareRobustCommand{\gredge}[1]{\mathbin{\tikz[baseline] \draw[\StrSubstitute{#1}{<}{angle 60}] (0pt, 0.7ex) -- (\edgelength, 0.7ex);}}
\newcommand{\gredge}[1]{%
  %% Replace arrow headings
  \def\substarrow{#1}
  \StrSubstitute{\substarrow}{<}{angle 60}[\substarrow]
  \StrSubstitute{\substarrow}{>}{angle 60}[\substarrow]
  %% Print edge
  \mathbin{\tikz[baseline] \draw[\substarrow] (0pt, 0.7ex) -- (\edgelength, 0.7ex);}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

<<preliminaries, echo=FALSE>>=
op.orig <-
options(SweaveHooks= list(fig=function() par(mar=c(5.1, 4.1, 1.1, 2.1))),
        width = 75, digits = 5,
        ## JSS         : prompt = "R> "
        ## Good looking:
        prompt = "> ", continue = "  "
        )
@
%% include your article here, just as usual
%% Note that you should use the \pkg{}, \proglang{} and \code{} commands.

\section{Introduction}
\label{sec:introduction}
%%THIS DOCUMENTATION IS STILL UNDER CONSTRUCTION! \\ %% FIXME

Understanding cause-effect relationships between variables is of primary
interest in many fields of science. Usually, experimental intervention is
used to find these relationships. In many settings, however, experiments
are infeasible because of time, cost or ethical constraints.

We therefore consider the problem of inferring causal information from
observational data. Under some assumptions, the algorithms
PC   \citep[see][]{SpirtesEtAl00},
FCI  \citep[see][]{SpirtesEtAl00,SpirtesMeekRichardson99},
RFCI \citep[see][]{rfci} and
GES  \citep[see][]{Chickering2002} can
infer information about the causal structure from observational data;
there also exists a generalization of GES to interventional data, GIES
\citep[see][]{HauserBuhlmann2012}. These
algorithms tell us which variables could or could not be a cause of
some variable of interest. They do not, however, give information about the
size of the causal effects. We therefore developed the IDA method
\citep{MaKaBu09}, which can infer bounds on causal effects based on
observational data under some assumptions and in particular that no hidden
and selection variables are present. IDA is a two step approach that
combines the PC algorithm and Pearl's backdoor criterion \citep{Pearl93},
which has been designed for DAGs without latent variables. IDA was validated on a
large-scale biological system \citep[see][]{NatMethods10}. Since the
assumption of no latent variables is a strong assumption when working with
real data and therefore often violated in practice, we generalized Pearl's
backdoor criterion \citep[see][]{MaCo2013-arc} to more general types of graphs,
i.e. CPDAGs, MAGs, and PAGs, that describe Markov equivalence classes of
DAGs with and without latent variables but without selection variables.

For broader use of these methods, well documented and easy to use software
is indispensable. We therefore wrote the R package \pkg{pcalg}, which
contains implementations of the algorithms PC, FCI, RFCI, GES and GIES,
as well as of the IDA method and the generalized Pearl's backdoor
criterion. The objective of this paper is to introduce the R package
\pkg{pcalg}, explain the range of functions on simulated data sets and
summarize some applications.

To get started, we show how two of the main functions (one for causal
structure learning and one for estimating causal effects from observational
data) can be used in a
typical application. Suppose we have a system described by some
variables and many observations of this system. Furthermore, assume
that it seems plausible that there are no hidden variables and no
feedback loops in the underlying causal system. The causal structure
of such a system can be conveniently represented by a directed acyclic
graph (DAG), where each node represents a variable and each directed edge
represents a direct cause. To fix ideas, we have simulated an example
data set with $p = 8$ continuous variables with Gaussian noise and $n
= 5000$ observations, which we will now analyze. First, we load the
package \pkg{pcalg} and the data set.


<<def-gmG, eval=FALSE, echo=FALSE>>=
## Used to generate the  'gmG'  Gaussian data originally:
require("pcalg")
set.seed(40)
p <- 8
n <- 5000
gGtrue <- randomDAG(p, prob = 0.3)
gmG <- list(x = rmvDAG(n, gGtrue), g = gGtrue)
@

<<exIntro1>>=
library("pcalg")
data("gmG")
@


In the next step, we use the function \code{pc()} to produce an estimate
of the underlying causal structure. Since this function is based on
conditional independence tests, we need to define two things. First,
we need a function that can compute conditional independence tests in
a way that is suitable for the data at hand. For standard data types
(Gaussian, discrete and binary) we provide predefined functions. See
the example section in the help file of \code{pc()} for more
details. Secondly, we need a summary of the data (sufficient
statistic) on which the conditional independence function can
work. Each conditional independence test can be performed at a certain
significance level \code{alpha}. This can be treated as a tuning
parameter. In the following code, we use the predefined function
\code{gaussCItest()} as conditional independence test and create the
corresponding sufficient statistic, consisting
of the correlation matrix of the data and the sample size. Then we use
the function \code{pc()} to estimate the causal structure and plot the
result.

%% Define the 'Iplot' chunk here and use it twice below:
<<Iplot, echo=FALSE, eval=FALSE>>=
stopifnot(require(Rgraphviz))# needed for all our graph plots
par(mfrow = c(1,2))
plot(gmG8$g, main = "") ; plot(pc.gmG, main = "")
@ % two plots side by sid

<<exIntro>>=
suffStat <- list(C = cor(gmG8$x), n = nrow(gmG8$x))
pc.gmG <- pc(suffStat, indepTest = gaussCItest,
             p = ncol(gmG8$x), alpha = 0.01)
@
\begin{figure}[htb]
  \centering
<<exIntroPlot, echo=FALSE, fig=TRUE>>=
<<Iplot>>
@
\caption{True underlying causal DAG (left) and estimated causal structure
  (right), representing a Markov equivalence class of DAGs that all encode
  the same conditional independence information. (Due to the large sample
  size, there were no sampling errors.)}
\label{fig:intro1}
\end{figure}

As can be seen in Fig.~\ref{fig:intro1}, there are directed and bidirected
edges in the estimated causal structure. The directed edges show the
presence and direction of direct causal effects. A bidirected edge means
that the PC-algorithm was unable to decide whether the edge orientation
should be $\gredge{<-}$ or $\gredge{->}$. Thus, bidirected edges
represent some uncertainty in the resulting model. They reflect the fact
that in general one cannot estimate a unique DAG from observational data,
not even with an infinite amount of data, since several DAGs can describe
the same conditional independence information.

On the inferred causal structure, we can estimate the causal effect
of an intervention. Denote the variable corresponding to node $i$ in the
graph by $V_i$. For example, suppose that, by external intervention, we
first set the variable $V_1$ to some value $\tilde{x}$, and then to the
value $\tilde{x}+1$.  The recorded average change in variable $V_6$
is the (total) causal effect of $V_1$ on $V_6$. More precisely, the causal
effect $C(V_1, V_6, \tilde{x})$ of $V_1$ from $V_1 = \tilde{x}$ on $V_6$ is
defined as
\begin{eqnarray*}
C(V_1, V_6, \tilde{x}) &=& E(V_1|\doop(V_6 = \tilde{x} + 1)) - E(V_1|\doop(V_6 = \tilde{x})) \ \mbox{or} \\
C(V_1, V_6, \tilde{x}) &=& \frac{\partial}{\partial x} E(V_1|\doop(V_6 = x))|_{x=\tilde{x}},
\end{eqnarray*}
where $\doop(V_6 = x)$ denotes Pearl's do-operator \citep[see][]{Pearl00}. If
the causal relationships are linear, these two expressions are equivalent
and do not depend on $\tilde{x}$.

Since the causal structure was not identified uniquely in our example, we
cannot expect to get a unique number for the causal effect. Instead, we get
a set of possible causal effects. This set can be computed by using the
function \code{ida()}. To provide full quantitative information, we need to
pass the covariance matrix in addition to the estimated causal structure.

<<exIntro2>>=
ida(1, 6, cov(gmG8$x), pc.gmG@graph)
@

Since we simulated the data, we know that the true value of the causal
effect is \Sexpr{gGtrue <- gmG8$g; round(causalEffect(gGtrue, 6, 1), 2)}. %$
Thus, one of the two
estimates is indeed close to the true value. Since both values are larger
than zero, we can conclude that variable $V_1$ has a positive causal effect
on variable $V_6$. Thus, we can always estimate a lower bound for the
absolute value of the causal effect. (Note that at this point we have no
p-value to control the sampling error.)

If we would like to know the effect of a unit increase in variable $V_1$ on
variables $V_4$, $V_5$ and $V_6$, we could simply call \code{ida()} three
times. However, a faster way is to call the function \code{idaFast()}, which
was tailored for such situations.

<<exIntro3>>=
idaFast(1, c(4,5,6), cov(gmG8$x), pc.gmG@graph)
@

Each row in the output shows the estimated set of possible causal effects
on the target variable indicated by the row names.  The true values for the
causal effects are \Sexpr{round(causalEffect(gGtrue, 4, 1),2)},
\Sexpr{round(causalEffect(gGtrue, 5, 1),2)},
\Sexpr{round(causalEffect(gGtrue, 6, 1),2)} for variables $V_4$, $V_5$ and
$V_6$, respectively. The first row, corresponding to variable $V_4$, quite
accurately indicates a causal effect that is very close to zero or no
effect at all. The second row of the output, corresponding to variable
$V_5$, is rather uninformative: Although one entry comes close to the true
value, the other estimate is close to zero. Thus, we cannot be sure if
there is a causal effect at all. The third row, corresponding to $V_6$ was
already discussed above.

\section{Methodological background}

In Section~\ref{sec:gm} we propose methods for estimating the causal
structure. In particular, we discuss algorithms for structure learning
\begin{itemize}
  \item in the absence of hidden variables from observational data
    such as PC \citep[see][]{SpirtesEtAl00},
    GES \citep[see][]{Chickering2002}, and the dynamic programming approach of
    \cite{Silander2006},

  \item from observational data accounting for hidden variables such as
    FCI \citep[see][]{SpirtesEtAl00, SpirtesMeekRichardson99}
    and RFCI \citep[see][]{rfci},

  \item in the absence of hidden variables from jointly observational
    and interventional data such as GIES \citep[see][]{HauserBuhlmann2012}.
\end{itemize}

In Section~\ref{sec:bounds} we first describe the IDA method
\citep[see][]{MaKaBu09} to obtain bounds on causal effects from observational
data when no latent and selection variables are present. This method is
based on first estimating the causal structure and then applying
do-calculus \citep[see][]{Pearl00}. We then propose the generalized Pearl's
backdoor criterion \citep[see][]{MaCo2013-arc} that works with DAGs, CPDAGs,
MAGs, and PAGs as input and it assumes that there are arbitrarly many latent
variables but no selection variables. This method is based on two steps:
first it checks if the total causal effect of one variable $X$ onto another
variables $Y$ is identifiable via the generalized backdoor criterion in the
given type of graph, and if this is the case it explicitly gives a set of
variables that satisfies the generalized backdoor criterion with respect to
$X$ and $Y$ in the given graph.

\subsection{Estimating causal structures with  graphical models} \label{sec:gm}
Graphical models
can be thought of as maps of dependence structures of a given probability
distribution or a sample thereof \citep[see for example][]{lauritzen}. In
order to illustrate the analogy, let us consider a road map. In order to be
able to use a road map, one needs two given factors. First, one needs the
physical map with symbols such as dots and lines. Second, one needs a
rule for interpreting the symbols. For instance, a railroad map and a map
for electric circuits might look very much alike, but their interpretation
differs a lot. In the same sense, a graphical model is a map. First, a
graphical model consists of a graph with dots, lines and potentially edge
marks like arrowheads or circles. Second, a graphical model always comes
with a rule for interpreting this graph. In general, nodes in the graph
represent (random) variables and edges represent some kind of dependence.

\subsubsection{Without hidden and selection variables}
An example of a graphical model is the DAG model. The physical map here is
a graph consisting of nodes and directed edges ($\gredge{<-}$ or
$\gredge{->}$). As a further restriction, the edges must be directed in a
way, so that it is not possible to trace a cycle when following the
arrowheads (i.e.,  no directed cycles). The interpretation rule is called
d-separation. This rule is a bit intricate and we refer the reader to
\cite{lauritzen} for more details. This interpretation rule can be used in
the following way: If two nodes $x$ and $y$ are d-separated by a set of
nodes $S$, then the corresponding random variables $V_{x}$ and $V_{y}$ are
conditionally independent given the set of random variables $V_{S}$. For the
following, we only deal with distributions whose list of conditional
independencies perfectly matches the list of d-separation relations of
some DAG; such distributions are called faithful. It has been shown
that the set of distributions that are faithful is the overwhelming
majority \citep{Meek95}, so that the assumption does not seem to be very
strict in practice.

Since the DAG model encodes conditional independencies, it seems plausible
that information on the latter helps to infer aspects of the former. This
intuition is made precise in the PC algorithm (see \cite{SpirtesEtAl00}; PC
stands for the initials of its inventors Peter Spirtes and Clark Glymour)
which was proven to reconstruct the structure of the underlying DAG model given a
conditional independence oracle up to its Markov equivalence class which is
discussed in more detail below. In practice, the conditional independence oracle is
replaced by a statistical test for conditional independence. For situations
without hidden variables and under some further conditions it has been
shown that the PC algorithm using statistical tests instead of an
independence oracle is computationally feasible and consistent even for very
high-dimensional sparse DAGs \citep[see][]{KaBu07a}.

As mentioned before, several DAGs can encode the same list of conditional
independencies. One can show that such DAGs must share certain
properties. To be more precise, we have to define a v-structure as the
subgraph $i \gredge{->} j \gredge{<-} k$ on the nodes $i$, $j$ and $k$ where
$i$ and $k$ are not adjacent (i.e., there is no edge between $i$ and
$k$). Furthermore, let the skeleton of a DAG be the graph that is obtained
by removing all arrowheads from the DAG. It was shown that two DAGs encode
the same conditional independence statements if and only if the
corresponding DAGs have the same skeleton and the same v-structures
\citep[see][]{VermaPearl90}. Such DAGs are called Markov-equivalent. In this way,
the space of DAGs can be partitioned into equivalence classes, where all
members of an equivalence class encode the same conditional independence
information. Conversely, if given a conditional independence oracle, one
can only determine a DAG up to its equivalence class. Therefore, the PC
algorithm cannot determine the DAG uniquely, but only the corresponding
equivalence class of the DAG.

An equivalence class can be visualized by a graph that has the same
skeleton as every DAG in the equivalence class and directed edges only
where all DAGs in the equivalence class have the same directed edge. Edges
that point into one direction for some DAGs in the equivalence class and in
the other direction for other DAGs in the equivalence class are visualized
by bidirected edges (sometimes, undirected edges are used instead). This
graph is called a completed partially directed acyclic graph, CPDAG
\citep{SpirtesEtAl00}, or essential graph \citep{AnderssonMadiganPerlman97}.

\begin{algorithm}[h]
\caption{Outline of the PC-algorithm}
\label{pc}
\begin{algorithmic}
\STATE \textbf{Input:} Vertex set V, conditional independence information,
significance level $\alpha$\\
\STATE \textbf{Output:} Estimated CPDAG $\hat{G}$, separation sets $\hat{S}$\\
%\STATE \textbf{EDGE TYPES:} $\gredge{->}$, $-$\\
\hspace*{4em} \textbf{Edge types:} $\gredge{->}$, $\gredge{-}$\\
\STATE \textbf{(P1)} Form the complete undirected graph on the vertex
set V\\
\STATE \textbf{(P2)} Test conditional independence given subsets of
adjacency sets at a given significance level $\alpha$ and delete edges if
conditional independent\\
\STATE \textbf{(P3)} Orient v-structures\\
\STATE \textbf{(P4)} Orient remaining edges.\\
\end{algorithmic}
\label{algo:pc}
\end{algorithm}

We now describe the PC-algorithm, which is shown in Algorithm~\ref{algo:pc}, in more detail. The PC-algorithm starts with a complete
undirected graph, $G_0$, as stated in \textbf{(P1)} of Algorithm~\ref{algo:pc}. In stage \textbf{(P2)}, a series of conditional independence
tests is done and edges are deleted in the following way. First, all pairs
of nodes are tested for marginal independence. If two nodes $i$ and $j$ are
judged to be marginally independent at level $\alpha$, the edge between
them is deleted and the empty set is saved as separation sets
$\hat{S}[i,j]$ and $\hat{S}[j,i]$. After all pairs have been tested for
marginal independence and some edges might have been removed, a graph
results which we denote by $G_1$. In the second step, all pairs of nodes
$(i,j)$ still adjacent in $G_1$ are tested for conditional independence
given any single node in adj$(G_1,i)\setminus \{j\}$ or
adj$(G_1,j)\setminus \{i\}$ (adj$(G,i)$ denotes the set of nodes in graph
$G$ that are adjacent to node $i$) . If there is any node $k$ such that $V_i$
and $V_j$ are conditionally independent given $V_k$, the edge between $i$ and
$j$ is removed and node $k$ is saved as separation sets (sepset)
$\hat{S}[i,j]$ and $\hat{S}[j,i]$. If all adjacent pairs have been tested
given one adjacent node, a new graph results which we denote by
$G_2$. The algorithm continues in this way by increasing the size of the
conditioning set step by step. The algorithm stops if all adjacency sets in
the current graph are smaller than the size of the conditioning set. The
result is the skeleton in which every edge is still undirected. Within
\textbf{(P3)}, each triple of vertices $(i,k,j)$ such that the pairs
$(i,k)$ and $(j,k)$ are each adjacent in the skeleton but $(i,j)$ are not
(such a triple is called an ``unshielded triple''),
is oriented based on the information saved in the conditioning sets
$\hat{S}[i,j]$ and $\hat{S}[j,i]$. More precisely, an unshielded triple
$i \gredge{-} k \gredge{-} j$ is oriented as $i
\gredge{->} k \gredge{<-} j$ if $k$ is not in $\hat{S}[j,i] =
\hat{S}[i,j]$. Finally, in \textbf{(P4)} it may be possible to orient some
of the remaining edges, since one can deduce that one of the two possible
directions of the edge is invalid because it introduces a new v-structure
or a directed cycle. Such edges are found by repeatedly applying rules
described in \cite{SpirtesEtAl00}, p.85. The resulting output is the
equivalence class (CPDAG) that describes the conditional independence
information in the data, in which every edge is either undirected or
directed. (To simplify visual presentation, undirected edges are depicted as
bidirected edges in the output as soon as at least one directed edge is
present. If no directed edge is present, all edges are undirected.)

It is known that the PC algorithm is order-dependent in steps
\textbf{(P2)}--\textbf{(P4)}, meaning that the output depends from the order
in which the variables are given. \cite{CoMa2013-arc} proposed several
modifications of the PC algorithm (see Sections \ref{sec:skel} and
\ref{sec:pc}) that partly or fully remove these order-dependence issues in
each step.

The PC algorithm presented so far is based on conditional independence
tests. Score-based methods form an alternative approach to causal
inference. They try to find a CPDAG that maximizes a \emph{score},
typically a model selection criterion, which is calculated from data.

One of the most popular scores is the Bayesian information criterion (BIC)
because its maximization leads to \emph{consistent} model selection in the
classical large-sample limit \citep{Haughton1988, Geiger2001}.  However,
computing its maximum is an NP-hard problem \citep{Chickering1996}.  An
exhaustive search is computationally infeasible due to the size of the
search space, the space of DAGs or CPDAGs, respectively.
\cite{Silander2006} have presented an exact dynamic programming algorithm
with an exponential time complexity.  Its execution is feasible for models
with a few dozen variables.

The greedy equivalence search (GES) of \cite{Chickering2002} makes the
maximization of the BIC computationally feasible for much larger graphs.
As the name of the algorithm implies, GES maximizes the BIC in a
\emph{greedy} way, but still guarantees consistency in the large-sample
limit.  It still has exponential-time complexity in the worst case, but
only polynomial complexity in the average case where the size of the
largest clique in a graph grows only logarithmically with the number of
nodes \citep{Grimmett1975}.

GES greedily optimizes the BIC in two phases:
\begin{itemize}
  \item In the \emph{forward phase}, the algorithm starts with the empty
    graph.  It then sequentially moves to larger CPDAGs by operations that
    correspond to adding single arrows in the space of DAGs.  This phase is
    aborted if no augmentation of the BIC is possible any more.

  \item In the \emph{backward phase}, the algorithm moves again into the
    direction of \emph{smaller} graphs by operations that correspond to
    removing single arrows in the space of DAGs.  The algorithm terminates
    as soon as no augmentation of the BIC is possible any more.
\end{itemize}
A key ingredient for the fast exploration of the search space in GES is an
evaluation of the greedy steps in a local fashion which avoids enumerating
all representative DAGs of an equivalence class and which exploits the
decomposability of the BIC score \citep{Chickering2002}.

A causal structure without feedback loops and without hidden or selection
variable can be visualized using a DAG where the edges indicate direct
cause-effect relationships.  Under some assumptions, \cite{Pearl00} showed
(Theorem 1.4.1) that there is a link between causal structures and graphical
models. Roughly speaking, if the underlying causal structure is a DAG, we
observe data generated from this DAG and then estimate a DAG model (i.e.,  a
graphical model) on this data, the estimated CPDAG represents the
equivalence class of the DAG model describing the causal structure. This
holds if we have enough samples and assuming that the true underlying
causal structure is indeed a DAG without latent or selection
variables. Note that even given an infinite amount of data, we usually
cannot identify the true DAG itself, but only its equivalence class. Every
DAG in this equivalence class can be the true causal structure.

\subsubsection{With hidden or selection variables}
When discovering causal relations from nonexperimental data, two
difficulties arise. One is the problem of hidden (or latent) variables:
Factors influencing two or more measured variables may not themselves be
measured. The other is the problem of selection bias: Values of unmeasured
variables or features may influence whether a unit is included in the data
sample.

In the case of hidden or selection variables, one could still visualize the
underlying causal structure with a DAG that includes all observed, hidden
and selection variables. However, when inferring the DAG from observational
data, we do not know all hidden and selection variables.

We therefore seek to find a structure that represents all conditional
independence relationships among the observed variables given the selection
variables of the underlying causal structure. It turns out that this is
possible. However, the resulting object is in general not a DAG for the
following reason. Suppose, we have a DAG including observed, latent and
selection variables and we would like to visualize the conditional
independencies among the observed variables only. We could marginalize out
all latent variables and condition on all selection variables. It turns out
that the resulting list of conditional independencies can in general not be
represented by a DAG, since DAGs are not closed under marginalization or
conditioning \citep[see][]{RichardsonSpirtes02}.

A class of graphical independence models that is closed under
marginalization and conditioning and that contains all DAG models is the
class of ancestral graphs. A detailed discussion of this class of graphs
can be found in \cite{RichardsonSpirtes02}. In this text, we only give a
brief introduction.

Ancestral graphs have nodes, which represent random variables and edges
which represent some kind of dependence. The edges can be either directed
($\gredge{<-}$ or $\gredge{->}$),
undirected ($\gredge{-}$) or bidirected ($\gredge{<->}$) (note that in the
context of ancestral graphs,
undirected and bidirected edges do \emph{not} mean the same). There are two
rules that restrict the direction of edges in an ancestral graph:
\begin{description}
  \item[1:] If $i$ and $j$ are joined by an edge with an arrowhead at $i$, then
    there is no directed path from $i$ to $j$. (A path is a sequence of
    adjacent vertices, and a directed path is a path along directed edges
    that follows the direction of the arrowheads.)
  \item[2:] There are no arrowheads present at a vertex which is an
    endpoint of an undirected edge.
\end{description}
Maximal ancestral graphs (MAG), which we will use from now on, also
obey a third rule:
\begin{description}
\item[3:] Every missing edge corresponds to a conditional independence.
\end{description}

The conditional independence statements of MAGs can be read off using the
concept of m-separation, which is a generalization the concept of
d-separation. Furthermore, part of the causal information in the underlying
DAG is represented in the MAG. If in the MAG there is an edge between node
$i$ and node $j$ with an arrowhead at node $i$, then there is no directed
path from node $i$ to node $j$ nor to any of the selection variables in the
underlying DAG (i.e., $i$ is not a cause of $j$ or of the selection
variables). If, on the other hand, there is a tail at node $i$, then there
is a directed path from node $i$ to node $j$ or to one of the selection
variables in the underlying DAG (i.e., $i$ is a cause of $j$ or of a
selection variable).

Recall that finding a unique DAG from an independence oracle is in general
impossible. Therefore, one only reports on the equivalence class of DAGs in
which the true DAG must lie. The equivalence class is visualized using a
CPDAG. The same is true for MAGs: Finding a unique MAG from an independence
oracle is in general impossible. One only reports on the equivalence class
in which the true MAG lies.

An equivalence class of a MAG can be uniquely represented by a partial
ancestral graph (PAG) \citep[see, e.g.,][]{Zhang08-orientation-rules}.
A PAG contains the following types of edges: $\gredge{o-o}$, $\gredge{o-}$,
$\gredge{o->}$, $\gredge{->}$, $\gredge{<->}$, $\gredge{-}$.
Roughly, the bidirected edges come from hidden variables, and the
undirected edges come from selection variables. The edges have the
following interpretation: (i) There is an edge between $x$ and $y$ if and
only if $V_x$ and $V_y$ are conditionally dependent given $V_S$ for all
sets $V_S$ consisting of all selection variables and a subset of the
observed variables; (ii) a tail on an edge means that this tail is present
in all MAGs in the equivalence class; (iii) an arrowhead on an edge means
that this arrowhead is present in all MAGs in the equivalence class; (iv) a
$\circ$-edgemark means that there is a at least one MAG in the equivalence class
where the edgemark is a tail, and at least one where the edgemark is an
arrowhead.

An algorithm for finding the PAG given an independence oracle is the FCI
algorithm (``fast causal inference''; see \cite{SpirtesEtAl00} and
\cite{fci}). The orientation rules of this algorithm were slightly
extended and proven to be complete in \cite{Zhang08-orientation-rules}. FCI
is very similar to PC but makes additional conditional independence tests
and uses more orientation rules (see Section~\ref{sec:fci} for more
details). We refer the reader to \cite{Zhang08-orientation-rules} or
\cite{rfci} for a detailed discussion of the FCI algorithm. It turns out
that the FCI algorithm is computationally infeasible for large graphs. The
RFCI algorithm (``really fast causal inference''; see \cite{rfci}), is much
faster than FCI. The output of RFCI is in general slightly less informative
than the output of FCI, in particular with respect to conditional
independence information. However, it was shown in \cite{rfci} that any
causal information in the output of RFCI is correct and that both FCI and
RFCI are consistent in (different) sparse high-dimensional
settings. Finally, in simulations the estimation performances of the
algorithms are very similar.

Since both these algorithms are build up from the PC algorithm, they are
also order-dependent, meaning that the output depends from the order
in which the variables are given. Starting from the solution proposed for
the PC algorithm, \cite{CoMa2013-arc} proposed several
modifications of the FCI and the RFCI algorithms (see Sections
\ref{sec:skel}, \ref{sec:fci}, and \ref{sec:rfci}) that partly or fully
remove these order-dependence issues in each of their steps.

\subsubsection{From a mixture of observational and interventional data}

We often have to deal with interventional data in causal inference.  In
cell biology for example, data is often measured in different mutants, or
collected from gene knockdown experiments, or simply measured under
different experimental conditions.  An intervention, denoted by Pearl's
do-calculus (see Section \ref{sec:introduction}), changes the joint
probability distribution of the system; therefore, data samples collected
from different intervention experiments are \emph{not} identically
distributed (although still independent).

The algorithms PC and GES both rely on the i.i.d.\ assumption and are not
suited for causal inference from interventional data.  The GIES algorithm,
which stands for ``greedy interventional equivalence search'', is a
generalization of GES to interventional data \citep[see][]{HauserBuhlmann2012}.
It does not only make sure that interventional
data points are handled correctly (instead of being wrongly treated as
observational data points), but also accounts for the improved
identifiablity of causal models under interventional data by returning an
\emph{interventional essential graph}.  Just as in the observational case,
an interventional essential graph is a partially directed graph
representing an (interventional) Markov equivalence class of DAGs: a
directed edge between two vertices stands for an arrow with common
orientation among all representatives of the equivalence class, an
undirected edge stands for an arrow that has different orientations among
different representative DAGs; for more details, see \cite{HauserBuhlmann2012}.

GIES traverses the search space of interventional essential graphs in a
similar way as GES traverses the search space of observational essential
graphs.  In addition, a new search phase was introduced by
\cite{HauserBuhlmann2012} with movements which correspond to turning single
arrows in the space of DAGs.

\subsection{Estimating bounds on causal effects} \label{sec:bounds}
One way of quantifying the causal effect of variable $V_x$ on $V_y$ is to
measure the state of $V_y$ if $V_x$ is forced to take value $V_x=x$ and compare
this to the value of $V_y$ if $V_x$ is forced to take the value $V_x=x+1$ or
$V_x=x+\delta$. If $V_x$ and $V_y$ are random variables, forcing $V_x=x$ could have
the effect of changing the distribution of $V_y$. Following the conventions
in \cite{Pearl00}, the resulting distribution after manipulation is denoted
by $P[V_y | \doop(V_x=x)]$. Note that this is different from the conditional
distribution $P[V_y | V_x=x]$. To illustrate this, imagine the following
simplistic situation. Suppose we observe a particular spot on
the street during some hour. The random variable $V_x$ denotes whether it
rained during that
hour ($V_x=1$ if it rained, $V_x=0$ otherwise). The random variable $V_y$ denotes
whether the street was wet at the end of that hour ($V_y=1$
if it was wet, $V_y=0$ otherwise). If we assume $P(V_x=1) = 0.1$ (rather dry
region), $P(V_y=1|V_x=1) = 0.99$ (the street is almost always still wet at the
end of the hour when it rained during that hour) and $P(V_y=1|V_x=0) = 0.02$
(other reasons for making the street wet are rare), we can compute the
conditional probability $P(V_x=1|V_y=1) = 0.85$. So, if we observe the street
to be wet, the probability that there was rain in the last hour is about
$0.85$. However, if we take a garden hose and force the street to be wet at
a randomly chosen hour, we get $P(V_x=1|\doop(V_y=1)) = P(V_x=1) = 0.1$. Thus, the
distribution of the random variable describing rain is quite different when
making an observation versus making an intervention.

Oftentimes, only the change of the target distribution under intervention
is reported. We use the change in mean, i.e.,  $\frac{\partial}{\partial x}
E[V_y|\doop(V_x=x)]$, as a general measure for the causal effect of $V_x$ on
$V_y$. For multivariate Gaussian random variables, $E[V_y|\doop(V_x=x)]$ depends
linearly on $x$. Therefore, the derivative is constant which means that the
causal effect does not depend on $x$, and can also be interpreted as
$E[V_y|\doop(V_x=x+1)] - E[V_y|\doop(V_x=x)]$. For binary random variables
(with domain $\{0,1\}$) we define the causal effect of $V_x$ on $V_y$ as
$E(V_y|\doop(V_x=1)) - E(V_y|\doop(V_x=0)) =
 P(V_y=1|\doop(V_x=1)) - P(V_y=1|\doop(V_x=0))$.

The goal in the remainder of this section is to estimate the effect of an
intervention if only observational data is available.

\subsubsection{Without hidden and selection variables}
If the causal structure is a known DAG and there are no hidden and
selection variables, \cite{Pearl00} (Th 3.4.1) suggested a set of inference rules
known as ``do-calculus'' whose
application transforms an expression involving a ``do'' into an expression
involving only conditional distributions. Thus, information on the
interventional distribution can be obtained by using information obtained
by observations and knowledge of the underlying causal structure.

Unfortunately, the causal structure is rarely known in practice. However,
as discussed in Section~\ref{sec:gm}, we can estimate the Markov
equivalence class of the true causal DAG. Taking this into account, we
conceptually apply the do-calculus on each DAG within the equivalence class
and thus obtain a possible causal effect for each DAG in the equivalence
class (in practice, we developed a local method that is faster but yields a
similar result; see Section~\ref{sec:ida} for more details). Therefore,
even if we have an infinite amount of observations we can in general report
on a multiset of possible causal values (it is a multiset rather than a set
because it can contain duplicate values). One of these values is the true
causal effect. Despite the inherent ambiguity, this result can still be
very useful when the multiset has certain properties (e.g.,  all values are much
larger than zero). These ideas are incorporated in the IDA method
(\textbf{I}ntervention calculus when the \textbf{D}AG is \textbf{A}bsent).

In addition to this fundamental limitation in estimating a causal effect,
errors due to finite sample size blur the result as with every statistical
method. Thus, we can typically only get an estimate of the set of possible
causal values. It was shown that this estimate is consistent in sparse
high-dimensional settings under some assumptions by \cite{MaKaBu09}.

It has recently been shown empirically that despite the described
fundamental limitations in identifying the causal effect uniquely and
despite potential violations of the underlying assumptions, the
method performs well in identifying the most important causal effects in a
high-dimensional yeast gene expression data set \citep[see][]{NatMethods10}.

\subsubsection{With hidden but no selection variables}

If the causal DAG is known and no latent and selection variables are
present, one can estimate causal effects from observational data using for
example Pearl's backdoor criterion, as done in IDA.

However, in practice the assumption of no latent variables is often
violated. Therefore, \cite{MaCo2013-arc} generalized Pearl's backdoor
criterion to more general types of graphs that describe Markov equivalence
classes of DAGs when allowing arbitrarily many latent but no selection
variables. This generalization works with DAGs, CPDAGs, MAGs, and PAGs as
input and it is based on a two step approach. In a first step, the causal
effect of one variable $X$ onto another variable $Y$ under investigation is
checked to be identifiable via the generalized backdoor criterion, meaning
that there exists a set of variables $W$ for which the generalized backdoor
criterion is satisfied with respect to $X$ and $Y$ in the given graph. If
the effect is indeed identifiable, in a second step the set $W$ is explicitly
given.

\subsection{Summary of assumptions}
For all proposed methods, we assume that the data is faithful to the
unknown underlying causal DAG. For the individual methods, further
assumptions are made.

\begin{description}
\item[PC algorithm:] No hidden or selection variables; consistent in
  high-dimensional settings (the number of variables grows with the sample
  size) if the underlying DAG is sparse, the data is multivariate normal
  and satisfies some regularity conditions on the partial correlations, and
  $\alpha$ is taken to zero appropriately.  See \cite{KaBu07a} for full
  details. Consistency in a standard asymptotic regime with a fixed number
  of variables follows as a special case.

\item[GES algorithm:] No hidden or selection variables; consistency in
  a standard asymptotic regime with a fixed number of variables
  \citep[see][]{Chickering2002}.

\item[FCI algorithm:] Allows for hidden and selection variables; consistent
  in high-dimensional settings if the so-called Possible-D-SEP sets
  \citep[see][]{SpirtesEtAl00} are sparse, the data is multivariate normal and
  satisfies some regularity conditions on the partial correlations, and
  $\alpha$ is taken to zero appropriately. See \cite{rfci} for full
  details. Consistency in a standard asymptotic regime with a fixed number
  of variables follows as a special case.

\item[RFCI algorithm:] Allows for hidden and selection variables; consistent
  in high-dimensional settings if the underlying MAG is sparse (this is a
  much weaker assumption than the one needed for FCI), the data is
  multivariate normal and satisfies some regularity
  conditions on the partial correlations, and $\alpha$ is taken to zero
  appropriately. See \cite{rfci} for full details. Consistency in
  a standard asymptotic regime with a fixed number of variables follows as
  a special case.

\item[GIES algorithm:] No hidden or selection variables; mix of observational
  and interventional data. Interventional data alone is sufficient if
  there is no variable which is intervened in \emph{all} data points.

\item[IDA:] No hidden or selection variables; all conditional expectations
  are linear; consistent in high-dimensional settings if the underlying DAG
  is sparse, the data is multivariate Normal and satisfies some regularity
  conditions on the partial correlations and conditional variances, and
  $\alpha$ is taken to zero appropriately. See \cite{MaKaBu09} for full
  details.

\item[Generalized Backdoor Criterion:] allows for arbitrarily many hidden
  but no selection variables. See \cite{MaCo2013-arc} for more details.
\end{description}

\section{Package pcalg}
This package has two goals. First, it is intended to provide fast, flexible
and reliable implementations of the PC, FCI, RFCI, GES and GIES algorithms for
estimating causal structures and graphical models. Second, it provides an
implementation of the IDA method, which estimates bounds on causal effects
from observational data when no causal structure is known and hidden or
selection variables are absent, and it also provides a genralization of
Pearl's backdoor criterion to DAGs, CPDAGs, MAGs, nad PAGs, when hidden but
no selection variables are allowed.

In the following, we describe the main functions of our package for
achieving these goals. The functions \code{skeleton()}, \code{pc()},
\code{fci()}, \code{rfci()}, \code{ges()}, \code{gies()} and \code{simy()}
are intended for estimating graphical models. The functions \code{ida()}
and \code{idaFast()} are intended for estimating causal effects from
observational data, and the function \code{backdoor()} is intended for
checking if a causal effect is identifiable or not using the generalized
backdoor criterion and if it is identifiable for estimating a set that
actually satisfies the generalized backdoor criterion.

Alternatives to this package for estimating graphical models in
\proglang{R} include: \cite{bnlearn, deal, gRain, gRbase} and \cite{gRc}.

\subsection{skeleton}\label{sec:skel}
The function \code{skeleton()} estimates the skeleton of a DAG without
latent and selection variables using the PC algorithm
(steps (P1) and (P2) in Algorithm~\ref{algo:pc}), and it estimates an initial
skeleton of a DAG with arbitrarily many latent and selection variables
using the FCI and the RFCI algorithms. The function can be called with the
following arguments
\par\vspace*{-1.2ex}
<<skeleton-args, echo=FALSE>>=
showF <- function(f, width = 80) {
    ## 'width': larger than default on purpose:
    nam <- deparse(substitute(f))
    stopifnot(is.function(f))
    attr(f, "source") <- NULL # if ...
    attr(f, "srcref") <- NULL
    ll <- capture.output(str(f, width=width))
    ll[1] <- sub("function *", nam, ll[1])
    writeLines(ll)
}
showF(skeleton)
@

As was discussed in Section~\ref{sec:gm}, the main task in finding the skeleton
is to compute and test several conditional independencies. To keep the
function flexible, \code{skeleton()} takes as argument a function
\code{indepTest()} that performs these conditional independence tests and
returns a p-value. All information that is needed in the conditional
independence test can be passed in the argument \code{suffStat}. The only
exceptions are the number of variables \code{p} and the significance level
\code{alpha} for the conditional independence tests, which are passed
separately. For convenience, we have preprogrammed versions of
\code{indepTest()} for Gaussian data (\code{gaussCItest()}), discrete data
(\code{disCItest()}), and binary data (\code{binCItest()}). Each of these
independence test functions needs different arguments as input, described
in the respective help files. For example, when using \code{gaussCItest()},
the input has to be a list containing the correlation matrix and the sample
size of the data.  In the following code, we estimate the skeleton on the
data set \code{gmG} (which consists of $p=8$ variables and $n=5000$
samples) and plot the results. The estimated skeleton and the true
underlying DAG are shown in Fig.~\ref{fig:skelExpl}.
\begin{figure}
  \centering
<<skelExpl1Plot, fig=TRUE>>=
## using  data("gmG", package="pcalg")
suffStat <- list(C = cor(gmG8$x), n = nrow(gmG8$x))
skel.gmG <- skeleton(suffStat, indepTest = gaussCItest,
                   p = ncol(gmG8$x), alpha = 0.01)
par(mfrow = c(1,2))
plot(gmG8$g, main = ""); plot(skel.gmG, main = "")
@
\caption{True underlying DAG (left) and estimated skeleton (right) fitted
  on the simulated Gaussian data set \texttt{gmG}.}
\label{fig:skelExpl}
\end{figure}

To give another example, we show how to fit a skeleton to the example
data set \code{gmD} (which consists of $p=5$ discrete
variables with 3, 2, 3, 4 and 2 levels and $n=10000$ samples). The
predefined test function \code{disCItest()} is based on the $G^2$
statistic and takes as input a list containing the data matrix, a
vector specifying the number of levels for each variable and an option
which indicates if the degrees of freedom must be lowered by one for
each zero count. Finally, we plot the result. The estimated skeleton
and the true underlying DAG are shown in Fig.~\ref{fig:skel2}.
\begin{figure}
  \centering
<<skelExp2Plot, fig=TRUE, echo=FALSE>>=
data("gmD")
suffStat <- list(dm = gmD$x, nlev = c(3,2,3,4,2), adaptDF = FALSE)
skel.gmD <- skeleton(suffStat, indepTest = disCItest,
                     p = ncol(gmD$x), alpha = 0.01)
par(mfrow= 1:2); plot(gmD$g, main = ""); plot(skel.gmD, main = "")
@
\caption{True underlying DAG (left) and estimated skeleton (right) fitted
  on the simulated discrete data set \code{gmD}.}
\label{fig:skel2}
\end{figure}

In some situations, one may have prior information about the underlying
DAG, for example that certain edges are absent or present. Such information
can be incorporated  into the algorithm via the arguments
\code{fixedGaps} (absent edges) and \code{fixedEdges} (present edges).
The information in \code{fixedGaps} and \code{fixedEdges} is used as
follows. The gaps given in \code{fixedGaps} are introduced in the very
beginning of the algorithm by removing the corresponding edges from the
complete undirected graph. Thus, these edges are guaranteed to be absent in the
resulting graph. Pairs $(i,j)$ in \code{fixedEdges} are skipped in
all steps of the algorithm, so that these edges are guaranteed to be
present in the resulting graph.

If \code{indepTest()} returns \code{NA} and the option \code{NAdelete} is
\code{TRUE}, the corresponding edge is deleted. If this option is
\code{FALSE}, the edge is not deleted.

The argument \code{m.max} is the maximum size of the conditioning sets that
are considered in the conditional independence tests.

Throughout, the function works with the column positions of the
variables in the adjacency matrix, and not with the names of the variables.

The PC algorithm is known to be order-dependent, in the sense that the
output depends on the order in which the variables are given. Therefore,
\cite{CoMa2013-arc} proposed a simple modification, called PC-stable, that yields
order-independent adjacencies in the skeleton. In this function we
implement their modified algorithm (the old order-dependent implementation
can be found in version 1.1-5).

Since the FCI and RFCI algorithms are build up from the PC algorithm, they
are also order-dependent in the skeleton. To resolve their order-dependence
issues in the skeleton is more involved, see \cite{CoMa2013-arc}. However, this
function estimates an initial order-independent skeleton in these
algorithms (for additional details on how to make the final skeleton of FCI
fully order-independent see \ref{sec:fci} and \cite{CoMa2013-arc}).

\subsection{pc} \label{sec:pc}

The function \code{pc()} implements all steps (P1) to (P4) of the PC
algorithm shown in display algorithm~\ref{algo:pc}. First, the skeleton is
computed using the function \code{skeleton()} (steps (P1) and (P2)). Then,
as many edges as possible are oriented (steps (P3) and (P4)). The function
can be called as
% with the following arguments.
% \code{pc(suffStat, indepTest, p, alpha, verbose = FALSE, fixedGaps = NULL, \\
% fixedEdges = NULL, NAdelete = TRUE, m.max = Inf, u2pd = "rand", \\
% conservative = FALSE)}
\par\vspace*{-1.2ex}
<<pc-args, echo=FALSE>>=
showF(pc)
@
\par\vspace*{-1ex}
where the arguments \code{suffStat}, \code{indepTest}, \code{p},
\code{alpha}, \code{fixedGaps}, \code{fixedEdges}, \code{NAdelete} and
\code{m.max} are identical to those of \code{skeleton()}.

The conservative PC algorithm (\code{conservative = TRUE}) is a slight
variation of the PC algorithm \citep[see][]{Ramsey06}. After the
skeleton is computed, all unshielded triplets $a \gredge{-} b \gredge{-} c$ are
checked in the following way. We test whether $V_a$ and $V_c$ are
independent conditioning on any subset of the neighbors of $a$ or any
subset of the neighbors of $c$. If $b$ is in no such conditioning set (and
not in the original sepset) or in all such conditioning sets (and in the
original sepset), the triple is marked as \emph{unambiguous}, no further
action is taken and the usual PC is continued. If, however, $b$ is in only some
conditioning sets, or if there was no subset $S$ of the adjacency set of
$a$ nor of $c$ such that $V_a$ and $V_c$ are conditionally independent
given $V_S$, the triple $a \gredge{-} b \gredge{-} c$ is marked as
\emph{ambiguous}. An ambiguous triple is not oriented as a
v-structure. Furthermore, no later orientation rule that needs to know
whether $a \gredge{-} b \gredge{-} c$ is a v-structure or not is
applied. Instead of using the conservative version, which is quite strict
towards the v-structures, \cite{CoMa2013-arc} introduced a less strict version
for the v-structures called majority rule. This adaptation can be called using
\code{maj.rule = TRUE}. In this case, the triple $a \gredge{-} b \gredge{-}
c$ is marked as \emph{ambiguous} if $b$ is in exactly 50 percent of such
conditioning sets, if it is in less than 50 percent it is set as a
v-structure, and if in more than 50 percent as a non v-structure, for more
details see \cite{CoMa2013-arc}. The usage of both the conservative and the
majority rule versions resolve the order-dependence issues of the
determination of the v-structures, see \cite{CoMa2013-arc} for more details.

Sampling errors (or hidden variables) can lead to conflicting information
about edge directions. For example, one may find that $a \gredge{-} b
\gredge{-} c$ and $b \gredge{-} c \gredge{-} d$
should both be directed as v-structures. This gives conflicting information
about the edge $b \gredge{-} c$, since it should be directed as $b
\gredge{<-} c$ in v-structure $a \gredge{->} b \gredge{<-} c$, while it
should be directed as $b \gredge{->} c$ in v-structure $b \gredge{->} c
\gredge{<-} d$. With the option \code{solve.confl = FALSE}, in such cases,
we simply overwrite the directions of the conflicting edge. In the example
above this means that we obtain $a \gredge{->} b \gredge{->} c \gredge{<-} d$ if
$a \gredge{-} b \gredge{-} c$ was visited first, and
$a \gredge{->} b \gredge{<-} c \gredge{<-} d$ if
$b \gredge{-} c \gredge{-} d$ was visited first, meaning that the final
orientation on the edge depends on the ordering in which the edges are
oriented. With the option \code{solve.confl = TRUE} (which is only
supported with option \code{u2pd = "relaxed"}), we first generate a list of
all (unambiguous) v-structures (in the example above $a \gredge{-} b
\gredge{-} c$ and $b \gredge{-} c \gredge{-} d$), and then we simply orient
them allow both directions on the edge $b \gredge{-} c$, namely we allow the
bi-directed edge $b \gredge{<->} c$ resolving the order-dependence issues
on the edge orientations. We denote bi-directed edges in the adjacency
matrix M of the graph as $M[b,c]=2$ and $M[c,b]=2$. In a similar way using
lists for the candidate edges for each orientation rule and allowing
bi-directed edges, the order-dependence issues in the orientation rules can
be solved. Note that bi-directed edges merely represents a conflicting
orientation and they should not to be interpreted causally. The usage of
these lists for the candidate edges and allowing bi-directed edges resolve
the order-dependence issues on the orientation of the v-structures and on
the edges using the three orientation rules, see \cite{CoMa2013-arc} for more
details.

Note that calling (\code{conservative = TRUE}) or \code{maj.rule = TRUE},
together with \code{solve.confl = TRUE} produces a fully order-independent
output, see \cite{CoMa2013-arc}.

Sampling errors, non faithfulness, or hidden variables can also lead to
invalid CPDAGs, meaning that there does not exist a DAG that has the same
skeleton and v-structures as the graph found by the algorithm. An example
of this is an undirected cycle consisting of the edges $a \gredge{-} b
\gredge{-} c \gredge{-} d$ and $d \gredge{-} a$. In this case it
is impossible to direct the edges without creating a cycle or a new
v-structure. The optional argument \code{u2pd} specifies what should be
done in such a situation. If it is set to \code{"relaxed"}, the algorithm simply
outputs the invalid CPDAG. If \code{u2pd} is set to \code{"rand"}, all direction
information is discarded and a random DAG is generated on the skeleton. The
corresponding CPDAG is then returned. If
\code{u2pd} is set to \code{"retry"}, up to 100 combinations of possible
directions of the ambiguous edges are tried, and the first combination that
results in a valid CPDAG is chosen. If no valid combination is found, an
arbitrary CPDAG is generated on the skeleton as with \code{u2pd = "rand"}.

As with the skeleton, the PC algorithm works with the column positions of the
variables in the adjacency matrix, and not with the names of the
variables. When plotting the object, undirected and bidirected edges are
equivalent.

As an example, we estimate a CPDAG of the Gaussian data used in the
example for the skeleton in Section~\ref{sec:skel}. Again, we choose
the predefined \code{gaussCItest()} as conditional independence
test and create the corresponding test statistic. Finally, we plot the
result. The estimated CPDAG and the true underlying DAG are shown in
Fig.~\ref{fig:pcFit1}.

\begin{figure}
  \centering
<<pcExpl-plot, fig=TRUE>>=
suffStat <- list(C = cor(gmG8$x), n = nrow(gmG8$x))
pc.fit <- pc(suffStat, indepTest=gaussCItest, p = ncol(gmG8$x), alpha = 0.01)
par(mfrow= c(1,2));  plot(gmG8$g, main = ""); plot(pc.fit, main = "")
@
\caption{True underlying DAG (left) and estimated CPDAG (right) fitted on the
  simulated Gaussian data set \code{gmG}.}
\label{fig:pcFit1}
\end{figure}

\subsection{ges} \label{sec:ges}

The PC algorithm presented in the previous section is based on conditional
independence tests.  To apply it, we first had to calculate a sufficient
statistic and to specify a conditional independence test function.  For the
score-based GES algorithm, we have to define a score object before applying
the inference algorithm.

A score object is an instance of a class derived from the base class
\code{Score}.  This base class is implemented as a virtual reference class.
At the moment, the \pkg{pcalg} package only contains classes derived from
\code{Score} for Gaussian data: \code{GaussL0penObsScore} for purely
observational data, and \code{GaussL0penIntScore} for a mixture of
observational and interventional data; for the GES algorithm, we only need
the first one here, but we will encounter the second one in Section
\ref{sec:gies} again.  The implementation of score classes for discrete
data is planned for future versions of the \pkg{pcalg} package.  However,
the flexible implementation using class inheritance allows the user to
implement own score classes for different scores.

The predefined score-class \code{GaussL0penObsScore} implements a an
$\ell_0$-penalized maximum-likelihood estimator for observational data from
a Gaussian causal model.  A instance is generated as follows:
<<obs-score-args, eval = FALSE>>=
score <- new("GaussL0penObsScore", data = matrix(1, 1, 1),
    lambda = 0.5*log(nrow(data)), intercept = FALSE, use.cpp = TRUE, ...)
@
The data matrix is provided by the argument \code{data}.  The penalization
constant is specified by \code{lambda}.  The default value of \code{lambda}
corresponds to the BIC score; the AIC score is realized by setting
\code{lambda} to $1$.  The argument \code{intercept} indicates whether the
model should allow for intercepts in the linear structural equations, or
for a non-zero mean of the multivariate normal distribution, which is
equivalent.  The last argument \code{use.cpp} indicates whether the
internal C++ library should be used for calculation, which is in most cases
the best choice due to velocity issues.

Once a score object is defined, the GES algorithm is called as follows:
<<ges-args, echo = FALSE>>=
showF(ges)
@
The argument \code{score} is a score object defined before.  The
argument \code{turning} indicates whether the novel turning phase
(see Section \ref{sec:gm}) not
present in the original implementation of \cite{Chickering2002} should be
used, and \code{maxdegree} can be used to bound the vertex degree of the
estimated graph.  More details can be found in the help file of
\code{ges()}.

In Fig.~\ref{fig:gesFit}, we re-analyze the data set used in the example of
Fig.~\ref{fig:pcFit1} with the GES algorithm.  The estimated graph is
exactly the same in this case.

\begin{figure}
  \centering
<<gesExpl-plot, fig=TRUE>>=
score <- new("GaussL0penObsScore", gmG8$x)
ges.fit <- ges(score)
par(mfrow=1:2); plot(gmG8$g, main = ""); plot(ges.fit$essgraph, main = "")
@
\caption{True underlying DAG (left) and essential graph (right) estimated
  with the GES algorithm fitted on the simulated Gaussian data set \code{gmG}.}
\label{fig:gesFit}
\end{figure}

\subsection{fci}\label{sec:fci}

The FCI algorithm is a generalization of the PC algorithm, in the sense
that it allows arbitrarily many latent and selection variables. Under the
assumption that the data are faithful to a DAG that includes all latent and
selection variables, the FCI algorithm estimates the equivalence class of
MAGs that describe the conditional independence relationships between the
observed variables given the selection variables.

The first part of the FCI algorithm is analogous to the PC algorithm. It
starts with a complete undirected graph and estimates an initial skeleton
using the function \code{skeleton()}, which produces an initial
order-independent skeleton. All edges of this skeleton are of the
form $\gredge{o-o}$. However, due to the presence of hidden variables, it
is no longer sufficient to consider only subsets of the adjacency sets of
nodes $x$ and $y$ to decide whether the edge $x \gredge{-} y$ should be
removed. Therefore, the initial skeleton may contain some superfluous
edges. These edges are removed in the next step of the algorithm which
requires some orientations. Therefore, the v-structures are determined
using the conservative method (see discussion on \code{conservative}
below). All potential v-structures $a \gredge{-} b \gredge{-} c$ are
checked in the following way. We test whether $V_a$ and $V_c$ are
independent conditioning on any subset of the neighbors of $a$ or any
subset of the neighbors of $c$. If $b$ is in no such conditioning set or in
all such conditioning sets, no further action is taken. If, however, $b$ is
in only some conditioning sets, the triple $a \gredge{-} b \gredge{-} c$ is
marked as \emph{ambiguous}. If $V_a$ is independent of $V_c$ given some $S$
in the skeleton (i.e., the edge $a \gredge{-} c$ dropped out), but $V_a$
and $V_c$ remain dependent given all subsets of neighbors of either $a$ or
$c$, we will call all such triples $a \gredge{-} b \gredge{-} c$
\emph{unambiguous}. This is because in the FCI, the true separating set
might be outside the neighborhood of either $a$ or $c$. An ambiguous triple
is not oriented as a v-structure. After the v-structures have been
oriented, Possible-D-SEP sets for each node in the graph are computed at
once. To decide whether edge $x \gredge{o-o} y$ should be removed, one performs
conditional independence tests of $V_x$ and $V_y$ given all subsets of
Possible-D-SEP($x$) and of Possible-D-SEP($y$) (see help file of function
\code{pdsep()}). The edge is removed if a conditional independence is
found. This will produce a fully order-independent final skeleton as
explained in \cite{CoMa2013-arc}. Subsequently, all edges are transformed into
$\gredge{o-o}$ again and the v-structures are newly determined (using
information in sepset).  Finally, as many undetermined edge marks (o) as
possible are determined using (a subset of) the 10 orientation rules given
by \cite{Zhang08-orientation-rules}.

The function can be called with the following arguments:
\par\vspace*{-1.2ex}
<<fci-args, echo=FALSE>>=
showF(fci, width=75)
@
\par\vspace*{-1ex}
where the arguments \code{suffStat}, \code{indepTest}, \code{p},
\code{alpha}, \code{fixedGaps}, \code{fixedEdges}, \code{NAdelete} and
\code{m.max} are identical to those in % function
\code{skeleton()}.

The argument \code{pdsep.max} indicates the maximum size of Possible-D-SEP
for which subsets are considered as conditioning sets in the conditional
independence tests. If the nodes \code{x} and \code{y} are adjacent in the
graph and the size of Possible-D-SEP(\code{x})$\setminus
\{$\code{x},\code{y}$\}$ is bigger than \code{pdsep.max} the edge is simply
left in the graph. Note that if \code{pdsep.max} is less than Inf, the
final PAG may be a supergraph than the one computed with \code{pdsep.max =
  Inf}, because less tests may have been performed in the former.

The option \code{rules} contains a logical vector of
length 10 indicating which rules should be used when directing edges, where
the order of the rules is taken from \cite{Zhang08-orientation-rules}.

The option \code{doPdsep} indicates whether Possible-D-SEP should be
computed for all nodes, and all
subsets of Possible-D-SEP are considered as conditioning sets in the
conditional independence tests, if not defined otherwise in
\code{pdsep.max}. If FALSE, Possible-D-SEP is not computed, so that the
algorithm simplifies to the Modified PC algorithm of \cite{SpirtesEtAl00}.

By setting the argument \code{biCC = TRUE}, Possible-D-SEP($a$, $c$) is
defined as the intersection of the original Possible-D-SEP($a$, $c$) and
the set of nodes that lie on a path between $a$ and $c$. This method uses
biconnected components to find all nodes on a path between nodes $a$ and
$c$. The smaller Possible-D-SEP sets lead to faster computing times, while
\cite{rfci} showed that the algorithm is identical to the original FCI
algorithm given oracle information on the conditional independence
relationships.

Conservative versions of FCI, Anytime FCI, and Adaptive Anytime FCI (see
below) are computed if the argument of \code{conservative} is
\code{TRUE}. After the final skeleton is computed, all potential
v-structures $a \gredge{-} b \gredge{-} c$ are checked in the following
way. We test whether $V_a$ and $V_c$ are independent conditioning on any
subset of the neighbors of $a$ or any subset of the neighbors of $c$. When
a subset makes $V_a$ and $V_c$ conditionally independent, we call it a
separating set. If $b$ is in no such separating set or in all such
separating sets, no further action is taken and the normal version of the
FCI, Anytime FCI, or Adaptive Anytime FCI algorithm is continued. If,
however, $b$ is in only some separating sets, the triple $a \gredge{-} b
\gredge{-} c$ is marked \emph{ambiguous}. If $V_a$ is independent of $V_c$
given some $S$ in the skeleton (i.e., the edge $a \gredge{-} c$ dropped
out), but $V_a$ and $V_c$ remain dependent given all subsets of neighbors
of either $a$ or $c$, we will call all triples $a \gredge{-} b \gredge{-}
c$ \emph{unambiguous}. This is because in the FCI algorithm, the true
separating set might be outside the neighborhood of either $a$ or $c$. An
ambiguous triple is not oriented as a v-structure. Furthermore, no further
orientation rule that needs to know whether $a \gredge{-} b \gredge{-} c$
is a v-structure or not is applied. Instead of using the conservative
version, which is quite strict towards the v-structures, \cite{CoMa2013-arc}
introduced a less strict version for the v-structures called majority
rule. This adaptation can be called using \code{maj.rule = TRUE}. In this
case, the triple $a \gredge{-} b \gredge{-} c$ is marked as
\emph{ambiguous} if and only if $b$ is in exactly 50 percent of such
separating sets or no separating set was found. If $b$ is in less than 50
percent of the separating sets it is set as a v-structure, and if in more
than 50 percent it is set as a non v-structure, for more details see
\cite{CoMa2013-arc}. \cite{CoMa2013-arc} showed that with both these
modifications, the final skeleton and the decisions about the v-structures
of the FCI algorithm are fully order-independent.

Using the argument \code{labels}, one can pass names for the vertices of
the estimated graph. By default, this argument is set to \code{NA}, in
which case the node names \code{as.character(1:p)} are used.

The argument \code{type} specifies the version of the FCI that has to be
used. Per default it is \code{normal} and so the normal FCI algorithm is
called. If set as \code{anytime}, the Anytime FCI \cite{Spirtes01-anytime}
is called and in this case \code{m.max} must be specified by the
user. The Anytime FCI algorithm can be viewed as a modification of the FCI
algorithm that only performs conditional independence tests up to and
including order m.max when finding the initial skeleton, using the function
\code{skeleton}, and the final skeleton, using the function
\code{pdsep}. Thus, Anytime FCI performs fewer conditional independence
tests than FCI. If set as \code{adaptive}, the Adaptive Anytime FCI
\cite{rfci} is called and in this case m.max is not used. The first part of
the algorithm is identical to the normal FCI described above. But in the
second part when the final skeleton is estimated using the function
\code{pdsep}, the Adaptive Anytime FCI algorithm only performs conditional
independence tests up to and including order m.max, where m.max is the
maximum size of the conditioning sets that were considered to determine the
initial skeleton using the function \code{skeleton}.

As an example, we estimate the PAG of a graph with five nodes using the
function \code{fci()} and the predefined function \code{gaussCItest()} as
conditional independence test. In Fig.~\ref{fig:fci} the true DAG and the
PAG estimated with \code{fci()} are shown. Random variable $V_1$ is
latent. We can read off that both $V_4$ and $V_5$ cannot be a cause of
$V_2$ and $V_3$, which can be confirmed in the true DAG.

\begin{figure}
  \centering
<<fciExpl-plot, echo=FALSE, fig=TRUE>>=
data("gmL")
suffStat1 <- list(C = cor(gmL$x), n = nrow(gmL$x))
pag.est <- fci(suffStat1, indepTest = gaussCItest,
               p = ncol(gmL$x), alpha = 0.01, labels = as.character(2:5))
par(mfrow = 1:2); plot(gmL$g, main = ""); plot(pag.est)
@
\caption{True underlying DAG (left) and estimated PAG
  (right), when applying the FCI and RFCI algorithms to the data set
  \code{gmL}. The output of FCI and RFCI is identical. Variable $V_1$ of the
  true underlying DAG is latent.}
\label{fig:fci}
\end{figure}

%% <<pcAlgo-class>>=
%% showClass("pcAlgo")
%% @

\subsection{rfci}\label{sec:rfci}
The function \code{rfci()} is rather similar to \code{fci()}. However, it
does not compute any Possible-D-SEP sets and thus does not make tests
conditioning on them. This makes \code{rfci()} much faster than
\code{fci()}. The orientation rule for v-structures and the orientation
rule for so-called discriminating paths (rule 4) were modified in order to
produce a PAG which, in the oracle version, is guaranteed to have correct
ancestral relationships.

The function can be called in the following way:
\par\vspace*{-1.2ex}
<<rfci-args, echo=FALSE>>=
showF(rfci)
@
\par\vspace*{-1ex}
where the arguments \code{suffStat}, \code{indepTest}, \code{p},
\code{alpha}, \code{fixedGaps}, \code{fixedEdges}, \code{NAdelete} and
\code{m.max} are identical to those in \code{skeleton()}.

The argument \code{rules} is similar to the one in
\code{fci} but modified to produce a PAG with correct ancestral
relationships, in the oracle version.

The first part of the RFCI algorithm is analogous to the PC and FCI
algorithm. It starts with a complete undirected graph and estimates an
initial skeleton using the function \code{skeleton}, which produces
an initial order-independent skeleton, see \code{skeleton} for more
details. All edges of this skeleton are of the form $\gredge{o-o} $. Due to
the presence of hidden variables, it is no longer sufficient to consider
only subsets of the neighborhoods of nodes \code{x} and \code{y} to decide
whether the edge \code{x o-o y} should be removed. The FCI algorithm
performs independence tests conditioning on subsets of Possible-D-SEP to
remove those edges. Since this procedure is computationally infeasible, the
RFCI algorithm uses a different approach to remove some of those
superfluous edges before orienting the v-structures and the discriminating
paths in orientation rule 4.

Before orienting the v-structures, we perform the following additional
conditional independence tests. For each unshielded triple $a \gredge{-} b
\gredge{-} c$ in the initial skeleton, we check if both $V_a$ and $V_b$ and
$V_b$ and $V_c$ are conditionally dependent given the separating of $a$ and
$c$ (sepset$(a,c)$). These conditional dependencies may not have been
checked while estimating the initial skeleton, since sepset$(a,c)$ does not
need to be a subset of the neighbors of $a$ nor of the neighbors of $c$. If
both conditional dependencies hold and $b$ is not in the sepset$(a,c)$, the
triple is oriented as a v-structure $a \gredge{->} b \gredge{<-} c$. On the
other hand, if an additional conditional independence relationship may be
detected, say $V_a$ is independent from $V_b$ given the sepset$(a,c)$, the
edge between $a$ and $c$ is removed from the graph and the set responsible
for that is saved in sepset$(a,b)$. The removal of an edge can destroy or
create new unshielded triples in the graph. To solve this problem we work
with lists \citep[for details see][]{rfci}.

Before orienting discriminating paths, we perform the following additional
conditional independence tests. For each triple $a \gredge{<-*} b
\gredge{o-*} c$ with $a \gredge{->} c$, the algorithm searches for a
discriminating path $p = \left<d, . . . , a,b,c\right>$ for $b$ of minimal
length, and checks that the vertices in every consecutive pair $(V_f,V_g)$
on $p$ are conditionally dependent given all subsets of sepset$(d,c)
\setminus {V_f,V_g}$. If we do not find any conditional independence
relationship, the path is oriented as in rule (R4). If one or more
conditional independence relationships are found, the corresponding edges
are removed, their minimal separating sets are stored.

Conservative RFCI can be computed if the argument of \code{conservative} is
\code{TRUE}. After the final skeleton is computed and the additional local
tests on all unshielded triples, as described above, have been done, all
potential v-structures $a \gredge{-} b \gredge{-} c$ are checked in the
following way. We test whether $V_a$ and $V_c$ are independent conditioning
on any subset of the neighbors of $a$ or any subset of the neighbors of
$c$. When a subset makes $V_a$ and $V_c$ conditionally independent, we call
it a separating set. If $b$ is in no such separating set or in all such
separating sets, no further action is taken and the normal version of the
RFCI algorithm is continued. If, however, $b$ is in only some separating
sets, the triple $a \gredge{-} b \gredge{-} c$ is marked
\emph{ambiguous}. If $V_a$ is independent of $V_c$ given some $S$ in the
skeleton (i.e., the edge $a \gredge{-} c$ dropped out), but $V_a$ and $V_c$
remain dependent given all subsets of neighbors of either $a$ or $c$, we
will call all triples $a \gredge{-} b \gredge{-} c$
\emph{unambiguous}. This is because in the RFCI algorithm, the true
separating set might be outside the neighborhood of either $a$ or $c$. An
ambiguous triple is not oriented as a v-structure. Furthermore, no further
orientation rule that needs to know whether $a \gredge{-} b \gredge{-} c$
is a v-structure or not is applied. Instead of using the conservative
version, which is quite strict towards the v-structures, \cite{CoMa2013-arc}
introduced a less strict version for the v-structures called majority
rule. This adaptation can be called using \code{maj.rule = TRUE}. In this
case, the triple $a \gredge{-} b \gredge{-} c$ is marked as
\emph{ambiguous} if and only if $b$ is in exactly 50 percent of such
separating sets or no separating set was found. If $b$ is in less than 50
percent of the separating sets it is set as a v-structure, and if in more
than 50 percent it is set as a non v-structure, for more details see
\cite{CoMa2013-arc}.

The implementation uses the stabilized skeleton \code{skeleton()},
which produces an initial order-independent skeleton. The final skeleton
and edge orientations can still be order-dependent, see \cite{CoMa2013-arc}.

As an example, we re-run the example from Section~\ref{sec:fci} and show
the PAG estimated with \code{rfci()} in Figure~\ref{fig:fci}. The PAG
estimated with \code{fci()} and the PAG estimated with \code{rfci()} are
the same.

<<def-rfciExpl-plot, eval=FALSE>>=
data("gmL")
suffStat1 <- list(C = cor(gmL$x), n = nrow(gmL$x))
pag.est <- rfci(suffStat1, indepTest = gaussCItest,
                p = ncol(gmL$x), alpha = 0.01, labels = as.character(2:5))
@

A note on implementation: As \code{pc()}, \code{fci()} and \code{rfci()}
are similar in the result they produce, their resulting values are of (S4)
class \code{pcAlgo} and \code{fciAlgo} (for both \code{fci()} and
\code{rfci()}), respectively. Both classes extend
the class (of their ``communalities'') \code{gAlgo}.

\subsection{gies and simy} \label{sec:gies}

As we noted in Section \ref{sec:gm}, the GIES algorithm is a generalization
of the GES algorithm to a mix of interventional and observational data.
Hence the usage of \code{gies()} is similar to that of \code{ges()} (see
Section~\ref{sec:ges}).  Actually, the function \code{ges()} is only an
internal wrapper for \code{gies()}.

A data set with jointly interventional and observational data points is
\emph{not} i.i.d.  In order to use it for causal inference, we must specify
the intervention target each data point belongs to.  This is done by
specifying the arguments \code{target} and \code{target.index} when
generating an instance of \code{GaussL0penIntScore} (see Section
\ref{sec:ges}):
<<int-score-args, eval = FALSE>>=
score <- new("GaussL0penIntScore", data = matrix(1, 1, 1),
    targets = list(integer(0)), target.index = rep(as.integer(1), nrow(data)),
    lambda = 0.5*log(nrow(data)), intercept = FALSE, use.cpp = TRUE, ...)
@
The argument \code{targets} is a list of all (mutually different) targets
that have been intervened in the experiments generating the data set.  The
allocation of sample indices to intervention targets is specified by the
argument \code{target.index}.  This is an integer vector whose first entry
specifies the index of the intervention target in the list \code{targets}
of the first data point, whose second entry specifies the target index of
the second data point, and so on.  An example is given in Figure~\ref{fig:giesFit}.

Once a score object for interventional data is defined, the GIES algorithm
is called as follows:
<<gies-args, echo = FALSE>>=
showF(gies)
@
Most arguments coincide with those of \code{ges()} (see Section
\ref{sec:ges}).  The only additional argument is \code{targets}: it takes
the same list of (unique) intervention targets as the constructor of the
class \code{GaussL0penIntScore} (see above).  This list of targets
specifies the space of corresponding interventional Markov equivalence
classes or essential graphs (see Section \ref{sec:gm}).

<<def-gmInt, eval=FALSE, echo=FALSE>>=
## Used to generate the  'gmInt'  Gaussian data originally:
set.seed(40)
p <- 8
n <- 5000
gGtrue <- randomDAG(p, prob = 0.3)
nodes(gGtrue) <- c("Author", "Bar", "Ctrl", "Goal", "V5", "V6", "V7", "V8")
pardag <- as(gGtrue, "GaussParDAG")
pardag$set.err.var(rep(1, p))
targets <- list(integer(0), 3, 5)
target.index <- c(rep(1, 0.6*n), rep(2, n/5), rep(3, n/5))

x1 <- rmvnorm.ivent(0.6*n, pardag)
x2 <- rmvnorm.ivent(n/5, pardag, targets[[2]],
  matrix(rnorm(n/5, mean = 4, sd = 0.02), ncol = 1))
x3 <- rmvnorm.ivent(n/5, pardag, targets[[3]],
  matrix(rnorm(n/5, mean = 4, sd = 0.02), ncol = 1))
gmInt <- list(x = rbind(x1, x2, x3),
  targets = targets,
  target.index = target.index,
  g = gGtrue)
@
<<load-gmInt, echo = FALSE>>=
data(gmInt)
n.tot <- length(gmInt$target.index)
n.obs <- sum(gmInt$target.index == 1)
n3 <- sum(gmInt$target.index == 2)
n5 <- sum(gmInt$target.index == 3)
@

The data set \code{gmInt} consists of \Sexpr{n.tot} data points sampled
from the DAG in Figure~\ref{fig:skelExpl}, among them \Sexpr{n.obs}
observational ones, \Sexpr{n3} originating from an intervention at vertex
$3$ and \Sexpr{n5} originating from an intervention at vertex $5$.  It can
be loaded by calling
<<load-gmInt2>>=
data(gmInt)
@
The underlying causal model (or its interventional essential graph,
respectively) is estimated in Figure~\ref{fig:giesFit}.

\begin{figure}
  \centering
<<gies-fit-plot, fig=TRUE, height = 3>>=
score <- new("GaussL0penIntScore", gmInt$x, targets = gmInt$targets,
             target.index = gmInt$target.index)
gies.fit <- gies(score)
simy.fit <- simy(score)
par(mfrow = c(1, 3)) ; plot(gmInt$g, main = "")
plot(gies.fit$essgraph, main = "")
plot(simy.fit$essgraph, main = "")
@
  \caption{The underlying DAG (left) and the essential graph estimated with
    the GIES algorithm (middle) and the dynamic programming approach of
    \cite{Silander2006} (right) applied on the simulated interventional
    Gaussian data set \code{gmInt}.  This data set contains data from
    interventions at vertices $3$ and $5$; accordingly, the orientation of
    all arrows incident to these two vertices becomes identifiable (see
    also Figure~\ref{fig:gesFit} for comparison with the observational
    case.}
  \label{fig:giesFit}
\end{figure}

As an alternative to GIES, we can also use the dynamic programming approach
of \cite{Silander2006} to estimate the interventional essential graph from
this interventional data set.  This algorithm is implemented in the
function \code{simy()} which has the same arguments as \code{gies()}.  As
noted in Section \ref{sec:gm}, this approach yields the \emph{exact}
optimum of the BIC score at the price of a computational complexity which
is exponential in the number of variables.  On the small example based on
$8$ variables, using this algorithm is feasible; however, it is not
feasible for more than approximately $20$ variables, depending on the
processor and memory of the machine.  In this example, we get exactly the
same result as with \code{gies()} (see Figure~\ref{fig:giesFit}).

\subsection{ida} \label{sec:ida}

To illustrate the function \code{ida()}, consider the following example. We
simulated 10000 samples from seven multivariate Gaussian random variables
with a causal structure given on the left of Fig.~\ref{fig:ida}. We assume
that the causal structure is unknown and want to infer the causal effect of
$V_2$ on $V_5$. First, we estimate the equivalence class of DAGs that
describe the conditional independence relationships in the data, using the
function \code{pc()} (see Section~\ref{sec:pc}).

%% Used to generate "gmI" IDA data
<<def-gmI, eval=FALSE, echo=FALSE>>=
set.seed(123)
p <- 7
n <- 10000
myDAG <- randomDAG(p, prob = 0.2)
datI <- rmvDAG(n, myDAG)
gmI <- list(x = datI, g = myDAG)
@

<<idaExpl1>>=
data("gmI")
suffStat <- list(C = cor(gmI$x), n = nrow(gmI$x))
pc.gmI <- pc(suffStat, indepTest=gaussCItest,
             p = ncol(gmI$x), alpha = 0.01)
@
Comparing the true DAG with the CPDAG in Fig.~\ref{fig:ida}, we see that
the CPDAG and the true DAG have the same skeleton. Moreover, the directed
edges in the CPDAG are also directed in that way in the true DAG. Three
edges in the CPDAG are bidirected. Recall that undirected and bidirected
edges bear the same meaning in a CPDAG, so they can be used interchangeably.
\begin{figure}
  \centering
<<idaExpl2, fig=TRUE, echo = FALSE>>=
par(mfrow = c(1,2))
plot(gmI$g, main = "")
plot(pc.gmI, main = "")
@
\caption{True DAG (left) and estimated CPDAG (right).}
\label{fig:ida}
\end{figure}

Since there are three undirected edges in the CPDAG, there might be up to
$2^3 = 8$ DAGs in the corresponding equivalence class. However, the
undirected edges $2 \gredge{-} 3 \gredge{-} 5$ can be oriented as a
new v-structure. As
mentioned in Section~\ref{sec:gm}, DAGs in the equivalence class must have
exactly the same v-structures as the corresponding CPDAG. Thus,
$2 \gredge{-} 3 \gredge{-} 5$ can only be oriented as
$2 \gredge{->} 3 \gredge{->} 5$, $2 \gredge{<-} 3 \gredge{<-} 5$
or $2 \gredge{<-} 3
\gredge{->} 5$, and not as $2 \gredge{->} 3 \gredge{<-} 5$. There is
only one remaining undirected edge ($1 \gredge{-} 6$), which can be oriented in
two ways. Thus, there are six valid DAGs (i.e.,  they have no new v-structures
and no directed cycles) and these form the equivalence class represented by
the CPDAG. In Fig.~\ref{fig:allDags}, all DAGs in the equivalence class are
shown. DAG 6 is the true DAG.

<<idaExpl3, echo = FALSE>>=
am.pdag <- wgtMatrix(pc.gmI@graph)
ad <- allDags(am.pdag, am.pdag, NULL)
gDag <- vector("list", nrow(ad))
for (i in 1:nrow(ad)) gDag[[i]] <- as(matrix(ad[i, ], 7, 7), "graphNEL")
par(mfrow = c(3,2))
for (i in 1:6) plot(gDag[[i]], main = paste("DAG",i))
@
\begin{figure}
  \centering % height ~= 3 x {default height}
%% FIXME (R): DAG 2 and 5 below have a larger font than the others:
<<plot-6DAGS, fig=TRUE, height=11, echo = FALSE>>=
sfsmisc::mult.fig(6)
for (i in 1:6) plot(gDag[[i]], main = paste("DAG",i))
@
\caption{All DAGs belonging to the same equivalence class as the true
  DAG shown in Fig.~\ref{fig:fci}.}
  %%~\ref{fig:ida.}}
\label{fig:allDags}
\end{figure}

For each DAG G in the equivalence class, we apply Pearl's do-calculus to
estimate the total causal effect of $V_x$ on $V_y$. Since we assume
Gaussianity, this can be done via a
simple linear regression: If $y$ is not a parent of $x$, we take the
regression coefficient of $V_x$ in the regression \code{lm(Vy ~ Vx +
  pa(Vx))}, where \code{pa(Vx)} denotes the parents of $x$ in the DAG G ($z$ is
called a parent of $x$ if G if G contains the edge $z \gredge{->} x$); if
$y$ is a parent of $x$ in G, we set the estimated causal effect to zero.

If the equivalence class contains $k$ DAGs, this yields $k$ estimated
total causal effects. Since we do not know which DAG is the true causal
DAG, we do not know which estimated total causal effect of $V_x$ on $V_y$ is
the correct one. Therefore, we return the entire multiset of $k$ estimated
effects.

In our example, there are six DAGs in the equivalence class. Therefore, the
function \code{ida()} (with \code{method = "global"}) produces six possible
values of causal effects, one for each DAG.
<<idaExpl4>>=
ida(2, 5, cov(gmI$x), pc.gmI@graph, method = "global", verbose = FALSE)
@

Among these six values, there are only two unique values: $-0.0049$ and
$0.5421$. This is because we compute \code{lm(V5 ~ V2 + pa(V2))}
for each
DAG and report the regression coefficient of $V_2$. Note that there are
only two possible parent sets of node $2$ in all six DAGs: In DAGs 3 and 6,
there are no parents of node $2$. In DAGs 1, 2, 4 and 5, however, the parent
of node $2$ is node $3$. Thus, exactly the same regressions are computed for
DAGs 3 and 6, and the same regressions are computed for DAGs 1, 2, 4 and
5. Therefore, we end up with two unique values, one of which occurs twice,
while the other occurs four times.

Since the data was simulated from a model, we know that the true value of the
causal effect of $V_2$ on $V_5$ is $0.5529$. Thus, one of the two unique values is
indeed very close to the true causal effect (the slight discrepancy is due
to sampling error).

The function \code{ida()} can be called as % with the following arguments.
% \code{ida(x.pos, y.pos, mcov, graphEst, method = "local", y.notparent = FALSE,\\
% verbose = FALSE, all.dags = NA)},
\par\vspace*{-1.2ex}
<<ida-args, echo=FALSE>>=
showF(ida)
@
\par\vspace*{-1ex}
where \code{x.pos} denotes the column position of the cause variable,
\code{y.pos} denotes the column position of the effect
variable, \code{mcov} is the covariance matrix of the original
data, and \code{graphEst} is a graph object describing the causal structure
(this could be given by experts or estimated by \code{pc()}).

If \code{method="global"}, the method is carried out as described above,
where all DAGs in the equivalence class of the estimated CPDAG are
computed. This method is suitable for small graphs (say, up to 10
nodes). The DAGs can (but need not) be precomputed using the function
\code{allDags()} and passed via argument \code{all.dags}.

If \code{method="local"}, we do not determine all DAGs in the equivalence
class of the CPDAG. Instead, we only consider the local neighborhood of $x$
in the CPDAG. In particular, we consider all possible directions of
undirected edges that have $x$ as endpoint, such that no new v-structure is
created. For each such configuration, we estimate the total causal effect
of $V_x$ on $V_y$ as above, using linear regression.

At first sight, it is not clear that such a local configuration corresponds
to a DAG in the equivalence class of the CPDAG, since it may be impossible
to direct the remaining undirected edges without creating a directed cycle
or a v-structure. However, \cite{MaKaBu09} showed
that there is at least one DAG in the equivalence class for each such local
configuration.  As a result, it follows that the multisets of total causal
effects of the \code{global} and the \code{local} method have the same unique
values. They may, however, have different multiplicities.

Recall, that in the example using the global method, we obtained two unique
values with multiplicities two and four yielding six numbers in total.
Applying the local method, we obtain the same unique values, but the
multiplicities are 1 for both values.
<<idaExpl5>>=
ida(2,5, cov(gmI$x), pc.gmI@graph, method = "local")
@

One can take summary measures of the multiset. For example, the minimum
absolute value provides a lower bound on the size of the true causal
effect: If the minimum absolute value of all values in the multiset is
larger than one, then we know that the size of the true causal
effect (up to sampling error) must be larger than one. The fact that the
unique values of the multisets of the \code{global} and \code{local}
method are identical implies that summary measures of the multiset that
only depend on the unique values (such as the minimum absolute value) can
be found using the local method.

In some applications, it is clear that some variable is definitively a cause
of other variables. Consider for example a bacterium producing
a certain substance, taking the amount of produced substance as response
variable. Knocking out genes in the bacterium might change the ability to
produce the substance. By measuring the expression levels of genes, we want
to know which genes have a causal effect on the product. In this setting,
it is clear that the amount of substance is the effect and the activity of
the genes is the cause. Thus in the causal structure, the response variable
cannot be a parent node of any variable describing the expression level of
genes. This background knowledge can be easily incorporated: By setting the
option \code{y.notparent = TRUE}, all edges in the CPDAG that have the
response variable as endpoint (no matter whether directed or undirected)
are overwritten so that they are oriented towards the response variable.

\subsection{idaFast}
In some applications it is desirable to estimate the causal effect of one
variable on a set of response variables. In these situations, the function
\code{idaFast()} should be used. Imagine for example, that we have data
on several variables, that we have no background knowledge about the causal
effects among the variables and that we want to estimate the causal effect of
each variable onto each other variable. To this end, we could consider for
each variable the problem: What is the causal effect of this variable on
all other variables. Of course, one could solve the problem by using \code{ida()} on
each pair of variables. However, there is a more efficient way which uses
the fact that a linear regression of a fixed set of explanatory variables
on several different response variables can be computed very efficiently.

The function \code{idaFast()} can be called with the following arguments
%\code{idaFast(x.pos, y.pos.set, mcov, graphEst)}.
\par\vspace*{-1.2ex}
<<idaFast-args, echo=FALSE>>=
showF(idaFast)
@
\par\vspace*{-1ex}
The arguments
\code{x.pos}, \code{mcov}, \code{graphEst} have the same meaning as
the corresponding arguments in \code{ida()}. The argument \code{y.pos.set}
is a vector containing the column positions of all response variables of
interest.

This call performs \code{ida(x.pos, y.pos, mcov, graphEst, method="local",
  y.notparent= FALSE, verbose=FALSE)} for all values of \code{y.pos} in
\code{y.pos.set} at the same time and in an efficient way. Note that the
option \code{y.notparent = TRUE} is not implemented.
%%- , since it is not
%%- clear how to do that efficiently without orienting all edges away from
%%- \code{y.pos.set} at the same time, which seems not to be desirable.

Consider the example from Section~\ref{sec:ida}, where we computed the
causal effect of $V_2$ on $V_5$. Now, we want to compute the effect of $V_2$
on $V_5$, $V_6$ and $V_7$ using \code{idaFast()} and compare the results with
the output of \code{ida()}. As expected, both methods lead to the same results.
<<ida-idaFast>>=
(eff.est1 <- ida(2,5, cov(gmI$x), pc.gmI@graph, method="local"))
(eff.est2 <- ida(2,6, cov(gmI$x), pc.gmI@graph, method="local"))
(eff.est3 <- ida(2,7, cov(gmI$x), pc.gmI@graph, method="local"))

(eff.estF <- idaFast(2, c(5,6,7), cov(gmI$x), pc.gmI@graph))
@

\subsection{backdoor} \label{sec:backdoor}

This function is a generalization of Pearl's backdoor criterion, see
\cite{Pearl93}, defined for directed acyclic graphs (DAGs), for single
interventions and single outcome variable to more general
types of graphs (CPDAGs, MAGs, and PAGs) that describe Markov equivalence
classes of DAGs with and without latent variables but without selection
variables, for more details see \cite{MaCo2013-arc}.

The motivation to find a set $W$ that satisfies the generalized backdoor
criterion with respect to $X$ and $Y$ in the given graph relies on the
result of the generalized backdoor adjustment that says: ``If a set of
variables $W$ satisfies the generalized backdoor criterion relative to $X$
and $Y$ in the given graph, then the causal effect of $X$ on $Y$ is
identifiable and is given by:" $P(Y|\text{do}(X = x)) = \sum_W
P(Y|X,W).P(W)$. This result allows to write post-intervention densities
(the one written using Pearl's do-calculus) using only observational
densities estimated from the data.

This function can be called in the following way:
\par\vspace*{-1.2ex}
<<backdoor-args, echo=FALSE>>=
showF(backdoor)
@
\par\vspace*{-1ex}

where \code{amat} is the adjacency matrix of the given graph, \code{x}
denotes the column position of the cause variable, \code{y} denotes the
column position of the effect variable, and \code{mcov} is the covariance
matrix of the original data.

The argument \code{type} specifies the type of graph of the given adjacency
matrix in \code{amat}. If the input graph is a DAG (\code{type="dag"}),
this function reduces to Pearl's backdoor criterion for single
interventions and single outcome variable, and the parents of $X$ in the
DAG satisfies the backdoor criterion unless $Y$ is a parent of
$X$. Therefore, if $Y$ is a parent of $X$, there is no set $W$ that
satisfies the generalized backdoor criterion relative to $X$ and $Y$ in the
DAG and NA is output. Otherwise, the causal effect is identifiable and a
set $W$ that satisfies the generalized backdoor criterion relative to $X$
and $Y$ in the DAG is given. If the input graph is a CPDAG $C$
(\code{type="cpdag"}), a MAG $M$, or a PAG $P$ (with both $M$ and $P$ not
allowing selection variables), this function first checks if the total
causal effect of $X$ on $Y$ is identifiable via the generalized backdoor
criterion (see \cite{MaCo2013-arc}, Theorem 4.1). If the effect is not
identifiable, the output is NA. Otherwise, an explicit set $W$ that
satisfies the generalized backdoor criterion relative to $X$ and $Y$ in the
given graph is found.

Note that if the set $W$ is equal to the empty set, the output is NULL.

At this moment this function is not able to work with PAGs estimated using
the \code{rfci} Algorithm.

It is important to note that there can be pair of nodes \code{x} and
\code{y} for which there is no set $W$ that satisfies the generalized
backdoor criterion, but the total causal effect might be identifiable via
some other technique.

To illustrate this function, we use as example the CPDAG displayed in
Figure 4, page 15 of \cite{MaCo2013-arc}. The R-code below is used to generate
a DAG \code{g} that belongs to the required equivalence class which is
uniquely represented by the estimated CPDAG \code{myCPDAG}.

<<backdoorExCPDAG1>>=
p <- 6
amat <- t(matrix(c(0,0,1,1,0,1, 0,0,1,1,0,1, 0,0,0,0,1,0,
                   0,0,0,0,1,1, 0,0,0,0,0,0, 0,0,0,0,0,0), 6,6))
V <- as.character(1:6)
colnames(amat) <- rownames(amat) <- V
edL <- vector("list",length=6)
names(edL) <- V
edL[[1]] <- list(edges=c(3,4,6),weights=c(1,1,1))
edL[[2]] <- list(edges=c(3,4,6),weights=c(1,1,1))
edL[[3]] <- list(edges=5,weights=c(1))
edL[[4]] <- list(edges=c(5,6),weights=c(1,1))
g <- new("graphNEL", nodes=V, edgeL=edL, edgemode="directed")

cov.mat <- trueCov(g)

myCPDAG <- dag2cpdag(g)
true.amat <- as(myCPDAG, "matrix")
## true.amat[true.amat != 0] <- 1
@

The DAG \code{g} and the CPDAG \code{myCPDAG} are shown in Figure~\ref{fig:backdoor}.
\begin{figure}
  \centering
<<backdoorExpl, fig=TRUE, echo = FALSE>>=
par(mfrow = c(1,2))
plot(g, main = "")
plot(myCPDAG, main = "")
@
\caption{True DAG (left) and estimated CPDAG (right).}
\label{fig:backdoor}
\end{figure}

Now, we want to check if the effect of $V_6$ on $V_3$ in the given CPDAG is
identifiable using \code{backdoor()} and if this is the case know which set
$W$ satisfies the generalized backdoor criterion. As explained in Example 4 in
\cite{MaCo2013-arc}, the causal effect of $V_6$ on $V_3$ in the CPDAG
\code{myCPDAG} is identifiable via the generalized backdoor criterion and
there is a set $W$ that satisfies the generalized criterion:

<<backdoorExCPDAG2>>=
backdoor(true.amat, 6, 3, type="cpdag")
@

\subsection{Using a user specific conditional independence test}

In some cases it might be desirable to use a user specific conditional
independence test instead of the provided ones. The \pkg{pcalg}
package allows the use of any conditional independence test defined by
the user. In this section, we illustrate this feature by using a
conditional independence test for Gaussian data that is not predefined
in the package.

The functions \code{skeleton()}, \code{pc()} and \code{fci()} all need the
argument \code{indepTest}, a function of the form
\code{indepTest(x, y, S, suffStat)} to test
conditional independence relationships.  This function must return
the p-value of the conditional independence test of $V_x$ and $V_y$ given
$V_S$ and some information on the data in the form of a sufficient
statistic (this might be simply the data frame with the original
data), where $x$, $y$, $S$ indicate column positions of the original data
matrix. We will show an example that illustrates how to construct such a
function.

A simple way to compute the partial correlation of $V_x$ and $V_y$ given $V_S$
for some data is to solve the two associated linear regression problems
\code{lm}($V_x \sim V_S$) and \code{lm}($V_y \sim V_S$), get the residuals, and
calculate the correlation between the residuals. Finally, a correlation
test between the residuals yields a p-value that can be returned. The
argument \code{suffStat} is an arbitrary object containing several pieces
of information that are all used within the function to produce the
p-value. In the predefined function \code{gaussCItest()} for example, a
list containing the correlation matrix and the number of observations is
passed. This has the advantage that any favorite (e.g.,  robust) method of
computing the correlation matrix can be used before partial correlations
are computed. Oftentimes, however, it suffices to just pass the
complete data set in \code{suffStat}. We choose this simple method in
our example. An implementation of the function \code{myCItest()} could look
like this.
<<turn-off-plus, echo=FALSE>>=
options(continue = " ") # MM: so we don't get the "+ " continuation lines
@
<<myCItest>>=
myCItest <- function(x,y,S, suffStat) {
  if (length(S) == 0) {
    x. <- suffStat[,x]
    y. <- suffStat[,y]
  } else {
    rxy <- resid(lm.fit(y= suffStat[,c(x,y)], x= cbind(1, suffStat[,S])))
    x. <- rxy[,1];  y. <- rxy[,2]
  }
  cor.test(x., y.)$p.value
}
@
We can now use this function together with \code{pc()}.
<<gaussCItest-ex, echo = FALSE>>=
suffStat <- list(C = cor(gmG8$x), n = 5000)
pc.gmG <- pc(suffStat, indepTest=gaussCItest, p = 8, alpha = 0.01)
@
<<myCItest-def-plot, eval=FALSE>>=
pc.myfit <- pc(suffStat = gmG8$x, indepTest = myCItest,
               p = 8, alpha = 0.01)
par(mfrow = c(1,2)); plot(pc.gmG, main = ""); plot(pc.myfit, main = "")
@
\begin{figure}[htb]
  \centering
<<myCItest-ex-plot, echo=FALSE, fig=TRUE>>=
<<myCItest-def-plot>>
@
\caption{The estimated CPDAGs using the predefined conditional independence
  test \code{gaussCItest()} (left) and the user
  specified conditional independence test \code{myCItest()} (right)
  are identical for the \code{gmG} data.}
\label{fig:userSpec}
\end{figure}
As expected, the resulting CPDAG (see Fig.~\ref{fig:userSpec}) is the same
as in Section~\ref{sec:pc} where we used the function \code{gaussCItest()} as
conditional independence test. Note however that using
\code{gaussCItest()} is considerably faster than using \code{myCItest()} (on
our computer
% 2010-09-24 : -- R 2.12.0 alpha
% lynne         : x86_64        Linux 2.6.34.6-47.fc13.x86_64 : 129.132.58.30
% model name	: AMD Phenom(tm) II X4 925 Processor
% cpu MHz	: 1600.000
% bogomips	: 5600.57
$0.059$ seconds using \code{gaussCItest()} versus $1.05$
seconds using \code{myCItest()}).
%% MM: Nach Verbesserung [lm.fit()] ist's immer noch deutlich langsame
%% -----   myCItest() noch wesentlich schneller geschrieben werden muss !
<<time-tests, eval=FALSE, echo=FALSE>>=
system.time(for(i in 1:10)
            pc.fit <- pc(suffStat, indepTest=gaussCItest, p = 8, alpha = 0.01))
      ##  User      System verstrichen
      ## 0.593       0.000       0.594
system.time(for(i in 1:10)
            pc.myfit <- pc(gmG8$x,  indepTest = myCItest,  p = 8, alpha = 0.01))
## Using  resid(lm(..)) twice:
     ##   User      System verstrichen
     ## 44.864       0.007      44.937
## Using resid(lm.fit(..)):
     ## 10.550       0.067      10.632
@

\section{Applications}
The \pkg{pcalg} package has been used for applications in epidemiology
\citep[see][]{ICF}, biology \citep[see][]{NatMethods10, nagarajan} and the
social sciences \citep[see][]{danenberg}. We will discuss two applications in
more detail below.

\subsection{Graphical Models and Causal Effects  in Human Functioning}
In \cite{ICF}, the development of WHO's International Classification of
Functioning, Disability and Health (ICF) on the one hand and recent
developments in graphical modeling on the other hand were combined to
deepen the understanding of human functioning. The objective of the paper
was to explore how graphical models can be
used in the study of ICF data. It was found that graphical models could be used
successfully for visualization of the dependence structure of the data set,
dimension reduction, and the comparison of subpopulations. Moreover,
estimations of bounds on causal effects using the IDA method yielded
plausible results. All analyses were done with the \pkg{pcalg} package.

\subsection{Causal effects among genes}
In \cite{NatMethods10}, the authors aim at quantifying the effects of
single gene interventions on the expression of other genes in yeast,
allowing for better insights into causal relations between genes. With $n =
63$ samples of observational data measuring the expression of $p = 5361$
genes \citep[see][]{HughesEtAl00}, the goal was to identify the largest
intervention effects between all pairs of genes. For the analysis, the
\pkg{pcalg} package with version 1.1-5 was used.

\cite{HughesEtAl00} also provide gene expression measurements from 234
interventional experiments, namely from 234 single-gene deletion mutant
strains. Using this data, we know the true causal effect of the knock-out
genes on the remaining genes in good approximation. We can then quantify
how well we can find the true intervention effects in the following way: We
encode the largest 10\% of the intervention effects computed from the
interventional data as the target set of effects that we want to
identify. We then check in an ROC curve, how well the ranking of the causal
effects estimated by applying \code{ida()} to the observational data is
able to identify effects in the target set. For comparison, the authors
also used the (conceptually wrong) Lasso and Elastic Net to obtain
rankings. In Figure~\ref{fig:yeast} one can see that \code{ida()}, using
the function \code{skeleton()} with version 1.1-5, is
clearly superior to the alternative methods (and random guessing) in terms
of identifying effects in the target set. In Figure~\ref{fig:yeast.new} one
can see that \code{ida()} using the stable function \code{skeleton()}
produces even better results than with the old version.

We note that the yeast data set is very high-dimensional ($n=63$,
$p=5361$). Thus, unlike the toy examples used to illustrate the package in
this manuscript, where $n$ was much bigger than $p$ and the causal
structure was recovered exactly up to its equivalence class, the estimated
causal structure for the yeast data is likely to contain many sampling
errors. However, Figure~\ref{fig:yeast} shows that it is still possible to
extract useful information about causal effects.

\begin{figure}[htb]
  \centering
  \includegraphics{Figure1FAT.pdf}
  \caption{The largest 10\% of the causal effects found in experiments
    among yeast genes are identified much better from observational data
    with IDA than with Lasso, Elastic Net or random guessing. The figure is
    essentially taken from \cite{NatMethods10}.}
\label{fig:yeast}
\end{figure}

\begin{figure}[htb]
  \centering
  \includegraphics{Figure2FAT.pdf}
  \caption{The largest 10\% of the causal effects found in experiments
    among yeast genes are identified much better from observational data
    with IDA than with Lasso, Elastic Net or random guessing. The figure is
    essentially taken from \cite{CoMa2013-arc}.}
\label{fig:yeast.new}
\end{figure}


\section{Discussion}

Causal structure learning and the estimation of causal effects from
observational data has large potential. However, we emphasize that we do
not propose causal inference methods based on observational data as a
replacement for experiments. Rather, IDA should be used as a guide for
prioritizing experiments, especially in situations where no clear
preferences based on the context can be given.

Data from cell biology is usually interventional data: different
measurements could originate from different mutants or different cell
stems, or from experiments with gene knockout or knockdown experiments.
The GIES algorithm is designed for causal inference from data of this
kind.

Since many assumptions of the proposed methods are uncheckable, it is
important to further validate the methods in a range of applications. We
hope that the \pkg{pcalg} package contributes to this important
issue by providing well-documented and easy to use software.

<<finalizing, echo=FALSE>>=
options(op.orig)
@
\newpage
\bibliography{Mybib}

\end{document}
